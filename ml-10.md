# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 10 Spectral Clustering</span>

### <span style="color: #6ED3C5">K-Means Clustering</span>

Given a set of data points $\{x_1, x_2, \ldots, x_n\}$ in $R^d$, the goal of K-Means clustering is to partition the data points into $k$ clusters $C_1, C_2, \ldots, C_k$ such that the within-cluster variance $\sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$ is minimized, where $\mu_i$ is the centroid of cluster $C_i$.

<span style="color: #6FA8FF">**Lloyd's Algorithm**</span> is a popular iterative method for K-Means clustering:

- randomly select $k$ initial centroids $\mu_1, \mu_2, \ldots, \mu_k$
- repeat until convergence:
  - assign each data point to the nearest centroid: $C_i = \{ x_j : ||x_j - \mu_i||^2 \leq ||x_j - \mu_l||^2 \text{ for all } l = 1, 2, \ldots, k \}$
  - recalculate the centroids of the clusters: $\mu_i = \dfrac{1}{|C_i|} \sum_{x_j \in C_i} x_j$

### <span style="color: #6ED3C5">Spectral Graph Clustering</span>

- $\epsilon$-neighborhood graph: connect points $x_i$ and $x_j$ if $||x_i - x_j|| < \epsilon$
- k-nearest neighbor graph: connect point $x_i$ to its $k$ nearest neighbors
- fully connected graph: connect all points with weights $w_{i,j}$

- adjacency matrix $A$ with weights $A_{ij} = w_{i,j}$
- degree matrix $D$ with $D_{ii} = \sum_{j} A_{ij}$
- unnormalized graph Laplacian $L = D - A$

<span style="color: #6FA8FF">**Theorem:**</span> Let $G$ be an undirected graph with connected components $A_1, A_2, \ldots, A_k$. Then the multiplicity of the eigenvalue 0 of the Laplacian matrix $L$ is equal to $k$, the number of connected components in the graph. The eigenspace corresponding to the eigenvalue 0 is spanned by the indicator vectors $I_{A_1}, I_{A_2}, \ldots, I_{A_k}$.

<span style="color: #6FA8FF">**Proof:**</span> $L$ is symmetric and positive semi-definite because for any vector $f \in R^n$,
$$f^T L f = \frac{1}{2} \sum_{i,j} w_{i,j} (f_i - f_j)^2 \geq 0.$$
If $G$ has exactly one connected component, then $L$ has an eigenvalue 0 with eigenvector $\mathbf{1}$ (the all-ones vector). If $f$ is an eigenvector corresponding to the eigenvalue 0, then
$$0 = f^T L f = \frac{1}{2} \sum_{i,j} w_{i,j} (f_i - f_j)^2,$$
which implies that $f_i = f_j$ for all connected nodes $i$ and $j$. Since the graph is connected, $f$ must be a constant vector, i.e., $f = c \mathbf{1}$ for some constant $c$. Thus, the eigenspace corresponding to the eigenvalue 0 is spanned by $\mathbf{1}$. If $G$ has $k$ connected components $A_1, A_2, \ldots, A_k$, then the Laplacian matrix $L$ can be block-diagonalized into $k$ blocks corresponding to each connected component. Each block has an eigenvalue 0 with eigenvector being the indicator vector of that component. Therefore, the multiplicity of the eigenvalue 0 is $k$, and the eigenspace is spanned by the indicator vectors $I_{A_1}, I_{A_2}, \ldots, I_{A_k}$.

<span style="color: #6FA8FF">**Spectral Clustering Algorithm:**</span>

1. Construct the similarity graph $G$ from the data points using an appropriate method (e.g., $\epsilon$-neighborhood, k-nearest neighbor, or fully connected graph with weights).
2. Compute the unnormalized graph Laplacian $L = D - A$.
3. (optional) Compute the normalized graph Laplacian $L_{sym} = D^{-1/2} L D^{-1/2}$ or $L_{rw} = D^{-1} L$.
4. Compute the first $k$ eigenvectors $u_1, u_2, \ldots, u_k$ of $L$ corresponding to the smallest $k$ eigenvalues. Use the power method with a spectral shift if necessary.
5. Form the matrix $U \in R^{n \times k}$ by stacking the eigenvectors as columns.
6. (optional) Normalize the rows of $U$ to have unit length.
7. Apply K-Means clustering to the rows of $U$ to obtain $k$ clusters.

<span style="color: #6FA8FF">**Graph Cut Problem:**</span> Given a graph $G = (V, E)$ with weights $w_{i,j}$ on edges, the goal is to partition the vertex set $V$ into $k$ disjoint subsets $C_1, C_2, \ldots, C_k$ such that the total weight of edges connecting different subsets is minimized. This can be formulated as minimizing the cut value $\text{Cut}(C_1, C_2, \ldots, C_k) = \dfrac{1}{2} \sum_{i=1}^{k} W(C_i, \bar{C_i})$, where $W(C_i, \bar{C_i})$ is the sum of weights of edges between $C_i$ and its complement $\bar{C_i}$.

<span style="color: #6FA8FF">**Ratio Cut Problem:**</span> To avoid unbalanced partitions, the Ratio Cut problem minimizes the ratio of the cut value to the size of the clusters:
$\text{RatioCut}(C_1, C_2, \ldots, C_k) = \sum_{i=1}^{k} \dfrac{W(C_i, \bar{C_i})}{|C_i|}.$

<span style="color: #6FA8FF">**Ratio Cut for $k=2$:**</span> The Ratio Cut can be expressed in terms of the graph Laplacian $L$ as follows:
Let $f \in R^n$ be defined as $f_i = \begin{cases}\sqrt{\dfrac{|\bar{C}|}{|C|}} & \text{if } i \in C \\ -\sqrt{\dfrac{|C|}{|\bar{C}|}} & \text{if } i \in \bar{C} \end{cases}$. Then,
$f^T L f = \dfrac{1}{2} \sum_{i,j} w_{i,j} (f_i - f_j)^2 = |V| \cdot \text{RatioCut}(C, \bar{C}).$
Minimizing the Ratio Cut is equivalent to minimizing $f^T L f$ subject to the constraints that $f$ is orthogonal to the all-ones vector and has a fixed norm $||f||^2 = n$. This leads to the relaxed problem of finding the second smallest eigenvector of $L$. Once the eigenvector is obtained, the data points can be partitioned based on the sign of the components of the eigenvector or by applying 2-Means clustering to the eigenvector components.

<span style="color: #6FA8FF">**Ratio Cut for $k>2$:**</span> The Ratio Cut can be expressed using indicator vectors for each cluster. Let $h_i \in R^n$ be defined as $h_i(j) = \begin{cases} \dfrac{1}{\sqrt{|C_i|}} & \text{if } j \in C_i \\ 0 & \text{otherwise} \end{cases}$. Then, $\text{RatioCut}(C_1, C_2, \ldots, C_k) = \text{tr}(H^T L H)$, where $H$ is the matrix with columns $h_1, h_2, \ldots, h_k$. Minimizing the Ratio Cut is equivalent to minimizing $\text{tr}(H^T L H)$ subject to the orthogonality constraint $H^T H = I$. The relaxed problem leads to finding the first $k$ eigenvectors of $L$. The data points can then be clustered by applying K-Means to the rows of the matrix formed by these eigenvectors.
