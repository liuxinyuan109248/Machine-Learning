# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 11 SimCLR, t-SNE, and RoPE</span>

### <span style="color: #6ED3C5">Contrastive Learning: SimCLR</span>

<span style="color: #6FA8FF">**InfoNCE Loss:**</span> Given a batch of $N$ samples, we create two augmented views for each sample, resulting in $2N$ samples. For a positive pair $(i, j)$ (two views of the same sample), the InfoNCE loss is defined as $-\log \dfrac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}$, where $\text{sim}(z_i, z_j)$ is the cosine similarity between the representations $z_i$ and $z_j$, and $\tau$ is a temperature parameter.

<span style="color: #6FA8FF">**Reproducing Kernel Hilbert Space (RKHS):**</span> A Hilbert space of functions where evaluation at each point can be represented as an inner product with a kernel function.

<span style="color: #6FA8FF">**Markov Random Field (MRF):**</span> A mechanism to model the joint distribution of a set of variables using an undirected graph, where nodes represent variables and edges represent dependencies.

- **score of $W$:** $s(W, \pi) = \prod_{(i,j) \in W} \pi_{ij}$
- **structural restriction:** $\Omega(W)$ is 1 if $W$ satisfies the restriction (e.g., being a matching), 0 otherwise
- **probability of $W$:** $p(W|\pi) = \dfrac{1}{Z(\pi)} s(W, \pi) \Omega(W)$

**Cross entropy loss for matching:** $H_\pi^k = - \mathbb{E}_{W_X \sim p(\cdot|\pi)} [\log p(W_Z = W_X)]$

<span style="color: #6FA8FF">**Contrastive Language-Image Pre-training (CLIP):**</span> A model that learns to associate images and text by training on a large dataset of image-caption pairs using contrastive learning.

### <span style="color: #6ED3C5">t-Distributed Stochastic Neighbor Embedding (t-SNE)</span>

A technique for dimensionality reduction that converts high-dimensional Euclidean distances into conditional probabilities representing similarities. It minimizes the Kullback-Leibler divergence between the joint probabilities of the high-dimensional data and the low-dimensional embedding.

<span style="color: #6FA8FF">**Stochastic Neighbor Embedding (SNE):**</span>

- **high-dimensional similarity:** $p_{j|i} = \dfrac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$
- **low-dimensional similarity:** $q_{j|i} = \dfrac{\exp(-||y_i - y_j||^2)}{\sum_{k \neq i} \exp(-||y_i - y_k||^2)}$
- **KL divergence loss:** $C = \sum_i \sum_j p_{j|i} \log \dfrac{p_{j|i}}{q_{j|i}}$

<span style="color: #6FA8FF">**Perplexity:**</span> A measure of the effective number of neighbors, defined as $2^{H(P_i)}$, where $H(P_i) = - \sum_j p_{j|i} \log_2 p_{j|i}$ is the Shannon entropy of the conditional probability distribution for point $i$. A binary search is used to find the appropriate $\sigma_i$ for each data point to achieve a desired perplexity.

<span style="color: #6FA8FF">**Perplexity in NLP:**</span> In natural language processing, perplexity is used to evaluate language models. It measures how well a probability model predicts a sample and is defined as the exponentiation of the average negative log-likelihood of the test set, i.e., $PP(W) = P(w_1, w_2, \ldots, w_N)^{-1/N} = \exp\left(-\dfrac{1}{N} \sum_{i=1}^N \log P(w_i | w_1, \ldots, w_{i-1})\right)$.

<span style="color: #6FA8FF">**t-Distributed Stochastic Neighbor Embedding (t-SNE):**</span>

- **high-dimensional similarity:** $p_{ij} = \dfrac{p_{j|i} + p_{i|j}}{2n}$
  - a single probability distribution: easier and faster to compute gradients
  - not heavy tails: avoid "crowding problem"
- **low-dimensional similarity:** $q_{ij} = \dfrac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}$
  - allow moderate distances in high-dimensional space to be modeled by larger distances in low-dimensional space
- **KL divergence loss:** $C = \sum_i \sum_j p_{ij} \log \dfrac{p_{ij}}{q_{ij}}$

### <span style="color: #6ED3C5">Rotary Positional Encoding (RoPE)</span>

<span style="color: #6FA8FF">**Rotary Positional Encoding (RoPE):**</span> A method for incorporating positional information into transformer models by applying a rotation to the query and key vectors based on their positions.

$$R_{\Theta, m}^d = \begin{bmatrix} \cos m\theta_1 & -\sin m\theta_1 & & & \\ \sin m\theta_1 & \cos m\theta_1 & & & \\ & & \ddots & & \\ & & & \cos m\theta_{d/2} & -\sin m\theta_{d/2} \\ & & & \sin m\theta_{d/2} & \cos m\theta_{d/2} \end{bmatrix}$$
$$f_q(q_m, m) = R_{\Theta, m}^d W_q q_m, \quad f_k(k_n, n) = R_{\Theta, n}^d W_k k_n, \quad \text{RoPE}(q_m, k_n) = f_q(q_m, m)^\top f_k(k_n, n) = q_m^\top W_q^\top R_{\Theta, n-m}^d W_k k_n$$
