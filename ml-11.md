# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 11 SimCLR, t-SNE, and RoPE</span>

### <span style="color: #6ED3C5">Contrastive Learning: The SimCLR Framework</span>

<span style="color: #6FA8FF">**SimCLR (Simple Framework for Contrastive Learning of Visual Representations)**</span> is a landmark self-supervised learning framework introduced by Google Research in 2020. It demonstrates that contrastive learning can achieve performance on par with supervised learning without requiring labeled data. The core idea is to learn representations by teaching a model to **recognize the same image under different transformations** while distinguishing it from other images.

- **Data Augmentation:** For every input image in a batch, SimCLR applies two different random augmentations (e.g., random cropping, color jittering, and Gaussian blur). This creates two "views" of the same image, forming a **positive pair**.
- <span style="color: #6FA8FF">**InfoNCE Loss:**</span> Given a batch of $N$ samples, we create two augmented views for each sample, resulting in $2N$ samples. For a positive pair $(i, j)$ (two views of the same sample), the InfoNCE loss is defined as $-\log \dfrac{\exp(\text{sim}(z_i, z_j)/2\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/2\tau)}$, where $\text{sim}(z_i, z_j)$ is the cosine similarity between the representations $z_i$ and $z_j$, and $\tau$ is a temperature parameter.

<span style="color: #6FA8FF">**Contrastive Language-Image Pre-training (CLIP)**</span> is a model that learns to associate images and text by training on a large dataset of image-caption pairs using contrastive learning.

To understand why SimCLR works, we bridge the gap between contrastive loss and structural matching via <span style="color: #6FA8FF">**Reproducing Kernel Hilbert Spaces (RKHS)**</span> and <span style="color: #6FA8FF">**Markov Random Fields (MRF)**</span>.

### <span style="color: #6ED3C5">Distance Metrics and Kernels</span>

<span style="color: #6FA8FF">**Reproducing Kernel Hilbert Space (RKHS):**</span> A Hilbert space of functions where evaluation at each point can be represented as an inner product with a kernel function. For a kernel $k(x, y)$, there exists a feature map $\phi(x)$ such that $k(x, y) = \langle \phi(x), \phi(y) \rangle$. The distance between two points in RKHS is given by $d_k(x, y) = \|\phi(x) - \phi(y)\|_{\mathcal{H}} = \sqrt{k(x, x) - 2k(x, y) + k(y, y)}$.

<span style="color: #6FA8FF">**Gaussian (RBF) Kernel:**</span> A commonly used kernel defined as $k(x, y) = \exp\left(-\dfrac{\|x - y\|^2}{2\sigma^2}\right)$, where $\sigma$ is a bandwidth parameter that controls the width of the kernel.

<span style="color: #6FA8FF">**Comparison of $\ell_2$-distance and RKHS Distance:**</span> The $\ell_2$-distance measures the absolute geometric separation. The RKHS distance (specifically with a radial basis function kernel) measures similarity. Once points are sufficiently far apart, they are considered "maximally different" (orthogonal in feature space), and the distance saturates (approaches $\sqrt{2}$), rather than increasing indefinitely.

### <span style="color: #6ED3C5">Structural Matching via Markov Random Fields (MRF)</span>

<span style="color: #6FA8FF">**Markov Random Field (MRF):**</span> A mechanism to model the joint distribution of a set of variables using an undirected graph, where nodes represent variables and edges represent dependencies.

- **score of $W$:** $s(W, \pi) = \prod_{(i,j) \in W} \pi_{ij}$
- **structural restriction:** $\Omega(W)$ is 1 if $W$ satisfies the restriction (e.g., being a matching), 0 otherwise
- **probability of $W$:** $p(W|\pi) = \dfrac{1}{Z(\pi)} s(W, \pi) \Omega(W)$

<span style="color: #6FA8FF">**Comparison of MRF Sampling and Direct Sampling:**</span> Direct sampling generates random noise where spatial structure is ignored. MRF sampling generates structured data (like smooth regions in images) because the value of a node is influenced by its local context.

<span style="color: #6FA8FF">**Cross-Entropy Loss for Structure Matching:**</span> We define the objective as minimizing the cross-entropy between the predicted latent distribution and the ground-truth matching. This measures how well the latent representations $Z$ can reconstruct the observed graph structure $W_X$: $H_\pi^k = - \mathbb{E}_{W_X \sim p(\cdot|\pi)} [\log p(W_Z = W_X)] = - \mathbb{E}_{W_X \sim p(\cdot|\pi)} \left[ \sum_{(i,j)} (W_X)_{ij} \log k(Z_i, Z_j)\right] + \log R(Z)$.

Using the Gaussian kernel $\log k(Z_i, Z_j) \propto -\dfrac{\|Z_i - Z_j\|^2}{2\sigma^2}$, we get:
$$ H_\pi^k = \sum_{(i,j)} \mathbb{E}_{W_X \sim p(\cdot|\pi)} \left[(W_X)_{ij} \cdot \frac{\|Z_i - Z_j\|^2}{2\sigma^2}\right] + \log R(Z) = \frac{1}{\sigma^2} \mathbb{E}_{W_X \sim p(\cdot|\pi)} [\text{tr}(Z^\top L(W_X) Z)] + \log R(Z), $$
where $L(W_X)$ is the Graph Laplacian defined as $L(W_X) = D(W_X) - W_X$, with $D(W_X)$ being the degree matrix. This formulation establishes a formal equivalence between probabilistic matching and constrained spectral clustering:

- **Equivalence to Spectral Clustering:** The first term is exactly the objective of spectral clustering. Minimizing the trace of the Laplacian forces connected nodes ($W_{ij} > 0$) to have proximal representations $Z_i, Z_j$, effectively performing a "soft" clustering where similar points are grouped together in the latent space.

- **Manifold Preservation:** It ensures that $Z$ preserves the local topological structure of the data manifold observed in $X$.

- **Prior Regularization:** The $\log R(Z)$ term serves as a "soft" alternative to the hard constraints typically used in clustering (like $Z^\top Z = I$). It prevents representation collapse (where all points map to a single coordinate) and enforces desired statistical properties on the learned embeddings.

When the structure $W_X$ is constrained to be a **perfect matching**—where each augmented view corresponds to exactly one original sample—this structural matching objective becomes mathematically equivalent to the **InfoNCE loss**. In this scenario, the trace of the Laplacian aligns the positive pairs, while the $\log R(Z)$ term (the partition function) corresponds to the denominator of the InfoNCE loss, ensuring the contrastive separation of negative pairs.

### <span style="color: #6ED3C5">Stochastic Neighbor Embedding (SNE)</span>

<span style="color: #6FA8FF">**Stochastic Neighbor Embedding (SNE)**</span> is a technique for dimensionality reduction that models high-dimensional data by converting Euclidean distances into conditional probabilities representing similarities. It minimizes the Kullback-Leibler divergence between the joint probabilities of the high-dimensional data and the low-dimensional embedding.

- **high-dimensional similarity:** $p_{j|i} = \dfrac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$
- **low-dimensional similarity:** $q_{j|i} = \dfrac{\exp(-||y_i - y_j||^2)}{\sum_{k \neq i} \exp(-||y_i - y_k||^2)}$
- **KL divergence loss:** $C = \sum_i \sum_j p_{j|i} \log \dfrac{p_{j|i}}{q_{j|i}}$

<span style="color: #6FA8FF">**Perplexity:**</span> A measure of the effective number of neighbors, defined as $2^{H(P_i)}$, where $H(P_i) = - \sum_j p_{j|i} \log_2 p_{j|i}$ is the Shannon entropy of the conditional probability distribution for point $i$. A binary search is used to find the appropriate $\sigma_i$ for each data point to achieve a desired perplexity.

<span style="color: #6FA8FF">**Perplexity in NLP:**</span> In natural language processing, perplexity is used to evaluate language models. It measures how well a probability model predicts a sample and is defined as the exponentiation of the average negative log-likelihood of the test set, i.e., $PP(W) = P(w_1, w_2, \ldots, w_N)^{-1/N} = \exp\left(-\dfrac{1}{N} \sum_{i=1}^N \log P(w_i | w_1, \ldots, w_{i-1})\right)$.

### <span style="color: #6ED3C5">t-Distributed Stochastic Neighbor Embedding (t-SNE)</span>

<span style="color: #6FA8FF">**t-Distributed Stochastic Neighbor Embedding (t-SNE)**</span> is an extension of SNE that addresses some of its limitations, particularly in preserving local structure and handling outliers. t-SNE introduces three key modifications:

**1. Symmetric SNE (Improved Joint Probability $p_{ij}$)**

In original SNE, $p_{j|i}$ is asymmetric ($p_{j|i} \neq p_{i|j}$). t-SNE simplifies this by defining a symmetric joint probability $p_{ij}$: $p_{ij} = \dfrac{p_{j|i} + p_{i|j}}{2n}$.

**Advantages over SNE:**

* <span style="color: #6FA8FF">**Easier Gradient Computation:**</span> The symmetric property leads to a much simpler and more computationally efficient gradient compared to the asymmetric version.
* <span style="color: #6FA8FF">**Outlier Robustness:**</span> In SNE, if a high-dimensional point $x_i$ is an outlier (far from all other points), all $p_{j|i}$ will be extremely small, causing $x_i$ to have almost no influence on the cost function. By using $p_{ij}$, every point $i$ makes a significant contribution to the cost function $\sum_j p_{ij} > \dfrac{1}{2n}$, ensuring outliers are not ignored.

**2. The <span style="color: #6FA8FF">Student t-Distribution</span> (Low-Dimensional Similarity $q_{ij}$)**

t-SNE replaces the Gaussian distribution used in SNE for the low-dimensional space with a heavy-tailed <span style="color: #6FA8FF">**Student t-distribution**</span> with one degree of freedom (which is a Cauchy distribution): $q_{ij} = \dfrac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}$.

**Advantages over SNE:**

* <span style="color: #6FA8FF">**Solving the Crowding Problem:**</span> In high-dimensional spaces, there is "more room" than in 2D or 3D. When mapping to lower dimensions, moderate distances in high-dimensional space cannot be faithfully preserved, often resulting in all points crushing together in the center (crowding).
* <span style="color: #6FA8FF">**Heavy Tails:**</span> Because the t-distribution has much "heavier tails" than a Gaussian, it allows moderately distant points in the high-dimensional space to be modeled by much larger distances in the low-dimensional space. This effectively pushes clusters apart and creates the clear "gaps" seen in t-SNE plots.

**3. KL Divergence Loss**

The cost function measures the difference between the high-dimensional distribution $P$ and low-dimensional distribution $Q$: $C = KL(P||Q) = \sum_{i \neq j} p_{ij} \log \dfrac{p_{ij}}{q_{ij}}$. The KL divergence is asymmetric. It places a high cost on modeling large $p_{ij}$ (nearby points in high-D) with small $q_{ij}$ (distant points in low-D). This means t-SNE prioritizes **local structure**: ensuring that points that were neighbors in the original space remain neighbors in the embedding.

### <span style="color: #6ED3C5">Rotary Positional Encoding (RoPE)</span>

<span style="color: #6FA8FF">**Rotary Positional Encoding (RoPE):**</span> A method for incorporating positional information into transformer models by applying a rotation to the query and key vectors based on their positions. This ensures that the attention score depends only on the **relative distance** $(m-n)$ between tokens.

$$R_{\Theta, m}^d = \begin{bmatrix} \cos m\theta_1 & -\sin m\theta_1 & & & \\ \sin m\theta_1 & \cos m\theta_1 & & & \\ & & \ddots & & \\ & & & \cos m\theta_{d/2} & -\sin m\theta_{d/2} \\ & & & \sin m\theta_{d/2} & \cos m\theta_{d/2} \end{bmatrix}$$
$$f_q(q_m, m) = R_{\Theta, m}^d W_q q_m, \quad f_k(k_n, n) = R_{\Theta, n}^d W_k k_n, \quad \text{RoPE}(q_m, k_n) = f_q(q_m, m)^\top f_k(k_n, n) = q_m^\top W_q^\top R_{\Theta, n-m}^d W_k k_n$$
