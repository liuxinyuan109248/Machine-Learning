# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 11 SimCLR, t-SNE, and RoPE</span>

### <span style="color: #6ED3C5">Contrastive Learning: SimCLR</span>

<span style="color: #6FA8FF">**InfoNCE Loss:**</span> Given a batch of $N$ samples, we create two augmented views for each sample, resulting in $2N$ samples. For a positive pair $(i, j)$ (two views of the same sample), the InfoNCE loss is defined as $-\log \dfrac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}$, where $\text{sim}(z_i, z_j)$ is the cosine similarity between the representations $z_i$ and $z_j$, and $\tau$ is a temperature parameter.

<span style="color: #6FA8FF">**Reproducing Kernel Hilbert Space (RKHS):**</span> A Hilbert space of functions where evaluation at each point can be represented as an inner product with a kernel function. For a kernel $k(x, y)$, there exists a feature map $\phi(x)$ such that $k(x, y) = \langle \phi(x), \phi(y) \rangle$. The distance between two points in RKHS is given by $d_k(x, y) = \|\phi(x) - \phi(y)\|_{\mathcal{H}} = \sqrt{k(x, x) - 2k(x, y) + k(y, y)}$.

<span style="color: #6FA8FF">**Gaussian (RBF) Kernel:**</span> A commonly used kernel defined as $k(x, y) = \exp\left(-\dfrac{\|x - y\|^2}{2\sigma^2}\right)$, where $\sigma$ is a bandwidth parameter that controls the width of the kernel.

<span style="color: #6FA8FF">**Comparison of $\ell_2$-distance and RKHS Distance:**</span> The $\ell_2$-distance measures the absolute geometric separation. The RKHS distance (specifically with a radial basis function kernel) measures similarity. Once points are sufficiently far apart, they are considered "maximally different" (orthogonal in feature space), and the distance saturates (approaches $\sqrt{2}$), rather than increasing indefinitely.

<span style="color: #6FA8FF">**Markov Random Field (MRF):**</span> A mechanism to model the joint distribution of a set of variables using an undirected graph, where nodes represent variables and edges represent dependencies.

- **score of $W$:** $s(W, \pi) = \prod_{(i,j) \in W} \pi_{ij}$
- **structural restriction:** $\Omega(W)$ is 1 if $W$ satisfies the restriction (e.g., being a matching), 0 otherwise
- **probability of $W$:** $p(W|\pi) = \dfrac{1}{Z(\pi)} s(W, \pi) \Omega(W)$

<span style="color: #6FA8FF">**Cross-Entropy Loss for Structure Matching:**</span> We define the objective as minimizing the cross-entropy between the predicted latent distribution and the ground-truth matching. This measures how well the latent representations $Z$ can reconstruct the observed graph structure $W_X$:
$$
\begin{aligned}
H_\pi^k =& - \mathbb{E}_{W_X \sim p(\cdot|\pi)} [\log p(W_Z = W_X)] = - \mathbb{E}_{W_X \sim p(\cdot|\pi)} \left[ \sum_{(i,j)} (W_X)_{ij} \log k(Z_i, Z_j)\right] = \sum_{(i,j)} \mathbb{E}_{W_X \sim p(\cdot|\pi)} \left[(W_X)_{ij} \cdot \frac{\|Z_i - Z_j\|^2}{2\sigma^2}\right] \\
=& \frac{1}{2\sigma^2} \mathbb{E}_{W_X \sim p(\cdot|\pi)} [2 \text{tr}(Z^\top L(W_X) Z)] = \frac{1}{\sigma^2} \mathbb{E}_{W_X \sim p(\cdot|\pi)} [\text{tr}(Z^\top L(W_X) Z)] \\
\end{aligned}
$$

<span style="color: #6FA8FF">**Unified Optimization Objective:**</span> The total objective for learning the latent representation $Z$ combines structural reconstruction with a prior constraint $R(Z)$: $\arg\min_Z \mathcal{L}(Z) = \arg\min_Z \left\{ \dfrac{1}{\sigma^2} \mathbb{E}_{W_X \sim p(\cdot|\pi)} [\text{tr}(Z^\top L(W_X) Z)] + \log R(Z) \right\}$. This formulation establishes a formal equivalence between probabilistic matching and constrained spectral clustering:

- **Equivalence to Spectral Clustering:** The first term is exactly the objective of spectral clustering. Minimizing the trace of the Laplacian forces connected nodes ($W_{ij} > 0$) to have proximal representations $Z_i, Z_j$, effectively performing a "soft" clustering where similar points are grouped together in the latent space.

- **Manifold Preservation:** It ensures that $Z$ preserves the local topological structure of the data manifold observed in $X$.

- **Prior Regularization:** The $\log R(Z)$ term serves as a "soft" alternative to the hard constraints typically used in clustering (like $Z^\top Z = I$). It prevents representation collapse (where all points map to a single coordinate) and enforces desired statistical properties on the learned embeddings.

<span style="color: #6FA8FF">**Comparison of MRF Sampling and Direct Sampling:**</span> Direct sampling generates random noise where spatial structure is ignored. MRF sampling generates structured data (like smooth regions in images) because the value of a node is influenced by its local context.

<span style="color: #6FA8FF">**Contrastive Language-Image Pre-training (CLIP):**</span> A model that learns to associate images and text by training on a large dataset of image-caption pairs using contrastive learning.

### <span style="color: #6ED3C5">t-Distributed Stochastic Neighbor Embedding (t-SNE)</span>

A technique for dimensionality reduction that converts high-dimensional Euclidean distances into conditional probabilities representing similarities. It minimizes the Kullback-Leibler divergence between the joint probabilities of the high-dimensional data and the low-dimensional embedding.

<span style="color: #6FA8FF">**Stochastic Neighbor Embedding (SNE):**</span>

- **high-dimensional similarity:** $p_{j|i} = \dfrac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$
- **low-dimensional similarity:** $q_{j|i} = \dfrac{\exp(-||y_i - y_j||^2)}{\sum_{k \neq i} \exp(-||y_i - y_k||^2)}$
- **KL divergence loss:** $C = \sum_i \sum_j p_{j|i} \log \dfrac{p_{j|i}}{q_{j|i}}$

<span style="color: #6FA8FF">**Perplexity:**</span> A measure of the effective number of neighbors, defined as $2^{H(P_i)}$, where $H(P_i) = - \sum_j p_{j|i} \log_2 p_{j|i}$ is the Shannon entropy of the conditional probability distribution for point $i$. A binary search is used to find the appropriate $\sigma_i$ for each data point to achieve a desired perplexity.

<span style="color: #6FA8FF">**Perplexity in NLP:**</span> In natural language processing, perplexity is used to evaluate language models. It measures how well a probability model predicts a sample and is defined as the exponentiation of the average negative log-likelihood of the test set, i.e., $PP(W) = P(w_1, w_2, \ldots, w_N)^{-1/N} = \exp\left(-\dfrac{1}{N} \sum_{i=1}^N \log P(w_i | w_1, \ldots, w_{i-1})\right)$.

<span style="color: #6FA8FF">**t-Distributed Stochastic Neighbor Embedding (t-SNE):**</span>

- **high-dimensional similarity:** $p_{ij} = \dfrac{p_{j|i} + p_{i|j}}{2n}$
  - a single probability distribution: easier and faster to compute gradients
  - not heavy tails: avoid "crowding problem"
- **low-dimensional similarity:** $q_{ij} = \dfrac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}$
  - allow moderate distances in high-dimensional space to be modeled by larger distances in low-dimensional space
- **KL divergence loss:** $C = \sum_i \sum_j p_{ij} \log \dfrac{p_{ij}}{q_{ij}}$

### <span style="color: #6ED3C5">Rotary Positional Encoding (RoPE)</span>

<span style="color: #6FA8FF">**Rotary Positional Encoding (RoPE):**</span> A method for incorporating positional information into transformer models by applying a rotation to the query and key vectors based on their positions.

$$R_{\Theta, m}^d = \begin{bmatrix} \cos m\theta_1 & -\sin m\theta_1 & & & \\ \sin m\theta_1 & \cos m\theta_1 & & & \\ & & \ddots & & \\ & & & \cos m\theta_{d/2} & -\sin m\theta_{d/2} \\ & & & \sin m\theta_{d/2} & \cos m\theta_{d/2} \end{bmatrix}$$
$$f_q(q_m, m) = R_{\Theta, m}^d W_q q_m, \quad f_k(k_n, n) = R_{\Theta, n}^d W_k k_n, \quad \text{RoPE}(q_m, k_n) = f_q(q_m, m)^\top f_k(k_n, n) = q_m^\top W_q^\top R_{\Theta, n-m}^d W_k k_n$$
