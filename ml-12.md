# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 12 Robust Machine Learning</span>

<span style="color: #6FA8FF">**Robust machine learning**</span> focuses on developing models that maintain high performance even when faced with adversarial inputs or perturbations. These perturbations can be small, imperceptible changes to the input data that can lead to significant misclassifications by the model. Robustness is crucial for deploying machine learning systems in real-world applications where data may be noisy or intentionally manipulated.

### <span style="color: #6ED3C5">Adversarial Training</span>

<span style="color: #6FA8FF">**Adversarial training**</span> is the most effective empirical defense against adversarial attacks. The goal is to train a model that remains accurate even when inputs are intentionally perturbed by an adversary.

Adversarial training is formulated as a **min-max problem** (or a saddle-point problem). We seek to find model parameters $\theta$ that minimize the risk under the worst-case perturbations: $\min_{\theta} \mathbb{E}_{(x,y) \sim D} \left[ \max_{\delta \in S} L(f_\theta(x + \delta), y) \right]$.

* **The Inner Maximization (The Attack):** Find the perturbation $\delta$ within a allowed set $S$ (usually an $\ell_p$-ball) that maximizes the loss. This simulates the strongest possible adversary for the current model.
* **The Outer Minimization (The Defense):** Update model parameters $\theta$ to minimize this "worst-case" loss, making the model robust to such attacks.

To solve the inner $\max$ term, we use gradient-based optimization in the input space:

* <span style="color: #6FA8FF">**Fast Gradient Sign Method (FGSM):**</span> An attack that moves in the direction of the gradient sign: $\delta = \epsilon \cdot \text{sign}(\nabla_x L(f_\theta(x), y))$.
* <span style="color: #6FA8FF">**Projected Gradient Descent (PGD):**</span> A multi-step iterative version of FGSM that provides a stronger first-order adversary. After each step, the perturbation is projected back onto the $\epsilon$-ball: $\delta_{t+1} = P_\Delta(\delta_t + \eta \nabla_x L(f_\theta(x + \delta_t), y))$, where $P_\Delta$ is the projection operator (clipping $\delta$ to $[-\epsilon, \epsilon]$).

The objective relies on <span style="color: #6FA8FF">**Danskin’s Theorem**</span>, which allows us to backpropagate through the maximum. If the inner problem is solved by an optimal $\delta^* = \arg\max_{\delta \in S} L(f_\theta(x + \delta), y)$, the gradient of the entire objective with respect to $\theta$ is simply the gradient of the loss evaluated at that perturbed point: $\nabla_\theta \mathbb{E}_{adv} = \mathbb{E}_{(x,y) \sim D} [\nabla_\theta L(f_\theta(x + \delta^*), y)]$.

<span style="color: #6FA8FF">**Adversarial Training Algorithm:**</span>

1. **Sample** a mini-batch of clean data $\{(x_i, y_i)\}$.
2. **Generate** adversarial examples $x'_i = x_i + \delta_i$ by solving the inner max (typically using **PGD**).
3. **Update** $\theta$ by performing a standard gradient descent step on the adversarial examples: $\theta \leftarrow \theta - \alpha \nabla_\theta \sum L(f_\theta(x'_i), y_i)$.

While adversarial training is robust, many other defenses rely on **Obfuscated Gradients**—techniques that make it difficult for an attacker to compute gradients without actually making the model robust. Common types include:

* **Shattered Gradients:** Using non-differentiable operations (e.g., <span style="color: #6FA8FF">**JPEG compression**</span>).
* **Stochastic Gradients:** Introducing randomness (e.g., random resizing).
* **Vanishing Gradients:** Deep networks or activations that cause gradients to zero out.

To evaluate the true robustness of such models, attackers often use the following strategies:

* <span style="color: #6FA8FF">**Backward Pass Differentiable Approximation (BPDA):**</span> 
    This technique is used when a model contains non-differentiable components (e.g., JPEG compression or quantization layers). The key idea is to use a differentiable approximation of the non-differentiable function during the backward pass of gradient computation.
* <span style="color: #6FA8FF">**Expectation Over Transformation (EOT):**</span>
    This strategy is used to defeat **Stochastic Gradients** (defenses that apply random transformations like resizing, padding, or noise to the input). The main idea is to compute the expected gradient over the distribution of random transformations applied by the defense.

### <span style="color: #6ED3C5">BPDA Attack on JPEG Compression</span>

The core challenge of attacking a model protected by **JPEG compression** is that the quantization step in JPEG creates a "staircase" function where the gradient is zero almost everywhere. BPDA bypasses this by using a surrogate gradient. The victim model can be represented as a composition of two functions:

* **$f(x)$**: The JPEG compression layer (Input $x \rightarrow$ Quantization/DCT $\rightarrow$ Output $z$).
* **$g(z)$**: The neural network classifier.
* **Total Prediction**: $y_{pred} = g(f(x))$.

BPDA recognizes that while $f(x)$ is non-differentiable, $f(x) \approx x$ (the image content remains visually similar).

* **Forward Pass**: Use the **actual, non-differentiable** JPEG function $f(x)$ to get the true classification loss.
* **Backward Pass**: When calculating the gradient $\nabla_x$, replace the non-differentiable $\dfrac{\partial f(x)}{\partial x}$ with the derivative of a differentiable approximation $h(x)$. Usually, $h(x)$ is the **identity function** ($h(x) = x$), so its derivative is simply **$1$**.

**BPDA Attack Algorithm:**

1.  **Input**: Original image $x$, target label $y$, and perturbation limit $\epsilon$.
2.  **Forward Propagation:** Compress the image $z = \text{JPEG}(x)$ and calculate loss $L = \text{Loss}(g(z), y)$.
3.  **Backward Propagation (The Trick)**:
    * Calculate gradient of loss with respect to the compressed input: $\nabla_z L$.
    * **Apply BPDA:** Set $\nabla_x L \approx \nabla_z L$, effectively treating the JPEG layer as transparent for the gradient.
4.  **Update Image:** Update pixels using the sign of the approximate gradient: $x_{adv} = x + \alpha \cdot \text{sign}(\nabla_x L)$.
5.  **Iterate**: Repeat until the image $x_{adv}$ successfully misleads the classifier even after being processed by the real JPEG algorithm.

### <span style="color: #6ED3C5">Certified Radius and Greedy Filling Algorithm</span>

**Certified Robustness:** Methods that provide formal guarantees on model robustness within a specified perturbation set

<span style="color: #6FA8FF">**Randomized Smoothing:**</span> Given a base classifier $f: \mathbb{R}^d \rightarrow \{1, \ldots, K\}$, the smoothed classifier $g$ is defined as: $g(x) = \arg\max_c \mathbb{P}_{\eta \sim \mathcal{N}(0, \sigma^2 I)}(f(x + \eta) = c)$.

<span style="color: #6FA8FF">**Neyman-Pearson Lemma:**</span>  To test a null hypothesis $H_0: X \sim q(x)$ against an alternative $H_1: X \sim p(x)$, the **most powerful test** at significance level $\alpha$ is defined by the rejection region: $R = \left\{ x : \dfrac{p(x)}{q(x)} > k \right\}$, where the threshold $k$ is chosen such that the probability of a false alarm matches $\alpha$: $P( \text{reject } H_0 \mid H_0 \text{ is true} ) = \int_R q(x) dx = \alpha$.

<span style="color: #6FA8FF">**Greedy Filling Algorithm:**</span> An algorithm to compute the certified radius for randomized smoothing by iteratively assigning points to classes based on likelihood ratios.

<span style="color: #6FA8FF">**Certified Radius for Randomized Smoothing:**</span> For a base classifier $f$ and input $x$, if $g(x) = c_A$ with probability $p_A$ and the second most probable class is $c_B$ with probability $p_B$, then the certified radius $R$ is given by $R = \dfrac{\sigma}{2} \left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right)$, where $\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function.

<span style="color: #6FA8FF">**Proof:**</span> **1. The Neyman-Pearson Lemma and the "Worst-case" Region**

To show $g(x+\delta) = c_A$, we must ensure that under the perturbed distribution, the probability of class $c_A$ remains greater than $c_B$. According to the **Neyman-Pearson Lemma**, given a fixed probability $p_A$ under the null distribution $q = \mathcal{N}(x, \sigma^2 I)$, the probability under the alternative distribution $p = \mathcal{N}(x + \delta, \sigma^2 I)$ is **minimized** when the decision region $S = \{ \eta : f(x+\eta) = c_A \}$ is a **half-space** that is "most distant" from the direction of the shift $\delta$. For Gaussian distributions, the likelihood ratio is
$$L(y) = \dfrac{p(y)}{q(y)} = \exp\left(\dfrac{\|y-x\|^2 - \|y-x-\delta\|^2}{2\sigma^2}\right) = \exp\left(\dfrac{2\langle y-x, \delta \rangle - \|\delta\|_2^2}{2\sigma^2}\right)$$
Thus, the worst-case region $S$ is: $S = \left\{ y \in \mathbb{R}^d : \left\langle y - x, \dfrac{\delta}{\|\delta\|_2} \right\rangle \leq \kappa \right\}$ where $\kappa$ is a constant determined by $p_A$.

**2. Lower Bounding $\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_A)$**

We project the $d$-dimensional Gaussian noise $\eta \sim \mathcal{N}(0, \sigma^2 I)$ onto the 1D line defined by the vector $\delta$. Let $Z \sim \mathcal{N}(0, 1)$ be a standard normal random variable. At the original point $x$, we have $\mathbb{P}(\eta \in S) = p_A \implies \mathbb{P}(\sigma Z \leq \kappa) = p_A$, which implies $\kappa = \sigma \Phi^{-1}(p_A)$. When the input is perturbed by $\delta$, the noise becomes $\eta' \sim \mathcal{N}(\delta, \sigma^2 I)$. In the direction of $\delta$, the random variable effectively becomes $\sigma Z + \|\delta\|_2$.
The probability of $c_A$ is lower-bounded by:
$$\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_A) \geq \mathbb{P}(\sigma Z + \|\delta\|_2 \leq \kappa) = \mathbb{P}\left(Z \leq \Phi^{-1}(p_A) - \frac{\|\delta\|_2}{\sigma}\right) = \Phi\left(\Phi^{-1}(p_A) - \frac{\|\delta\|_2}{\sigma}\right)$$

**3. Upper Bounding $\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_B)$**

To ensure the most difficult case for $c_A$ staying the leader, we assume $c_B$ occupies the remaining probability space in the direction that benefits most from the shift $\delta$. By symmetry: $\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_B) \leq \Phi\left(\Phi^{-1}(p_B) + \dfrac{\|\delta\|_2}{\sigma}\right)$.

**4. Deriving the Certified Radius $R$**

The prediction $g(x + \delta)$ is guaranteed to be $c_A$ if $\Phi\left(\Phi^{-1}(p_A) - \dfrac{\|\delta\|_2}{\sigma}\right) > \Phi\left(\Phi^{-1}(p_B) + \dfrac{\|\delta\|_2}{\sigma}\right)$. Since $\Phi$ is strictly monotonic, this is equivalent to $\|\delta\|_2 < \dfrac{\sigma}{2} \left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right)$. Thus, the certified radius $R$ is: $R = \dfrac{\sigma}{2} \left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right)$.
