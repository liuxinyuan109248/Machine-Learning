# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 12 Robust Machine Learning</span>

### <span style="color: #6ED3C5">Adversarial Attacks</span>

**Adversarial Training Objective:** $\mathbb{E}_{adv} = \mathbb{E}_{(x,y) \sim D} [\max_{\delta \in S} L(f_\theta(x + \delta), y)]$

<span style="color: #6FA8FF">**Fast Gradient Sign Method (FGSM):**</span> $\delta = \epsilon \cdot \text{sign}(\nabla_x L(f_\theta(x), y))$

<span style="color: #6FA8FF">**Projected Gradient Descent (PGD):**</span> $\delta_{t+1} = P_\Delta(\delta_t + \eta \nabla_x L(f_\theta(x + \delta_t), y))$ where $\Delta = \{\delta : ||\delta||_\infty \leq \epsilon\}$ and $P_\Delta(\delta) = \text{Clip}(\delta, [-\epsilon, \epsilon])$

<span style="color: #6FA8FF">**Danskin's Theorem:**</span> If $g(x) = \max_{z \in Z} f(x, z)$ and $f$ is differentiable, then $\nabla g(x) = \nabla_x f(x, z^*)$ where $z^* = \arg\max_{z \in Z} f(x, z)$. $\implies \nabla_\theta \mathbb{E}_{adv} = \mathbb{E}_{(x,y) \sim D} [\nabla_\theta L(f_\theta(x + \delta^*), y)]$ where $\delta^* = \arg\max_{\delta \in S} L(f_\theta(x + \delta), y)$

<span style="color: #6FA8FF">**Adversarial Training Algorithm:**</span>

1. Sample mini-batch $\{(x_i, y_i)\}_{i=1}^m$ from $D$
2. For each $(x_i, y_i)$, compute adversarial perturbation $\delta_i$ using PGD
   - Initialize $\delta_i^{(0)} = 0$
   - For $t = 0$ to $T-1$: $\delta_i^{(t+1)} = P_\Delta(\delta_i^{(t)} + \eta \nabla_x L(f_\theta(x_i + \delta_i^{(t)}), y_i))$
   - Set $\delta_i = \delta_i^{(T)}$
3. Update model parameters: $\theta \leftarrow \theta - \dfrac{\eta}{m} \nabla_\theta \sum_{i=1}^m L(f_\theta(x_i + \delta_i), y_i)$

**Obfuscated Gradients:** Techniques that make gradient-based attacks less effective without truly improving robustness

- **Shattered Gradients:** Non-differentiable operations that hinder gradient computation
- **Stochastic Gradients:** Randomized defenses that introduce noise in gradient estimation
- **Vanishing/Exploding Gradients:** Numerical issues that lead to unreliable gradient information

**Attack Strategies Against Obfuscated Gradients:**

- <span style="color: #6FA8FF">**Backward Pass Differentiable Approximation (BPDA):**</span> Replace non-differentiable components with differentiable approximations during backpropagation (e.g., replace a quantization step with an identity function)
- <span style="color: #6FA8FF">**Expectation Over Transformation (EOT):**</span> Average gradients over multiple stochastic transformations to obtain a more reliable estimate to perform attacks

**Certified Robustness:** Methods that provide formal guarantees on model robustness within a specified perturbation set

<span style="color: #6FA8FF">**Randomized Smoothing:**</span> $g(x) = \arg\max_c \mathbb{P}_{\eta \sim \mathcal{N}(0, \sigma^2 I)}(f(x + \eta) = c)$

<span style="color: #6FA8FF">**Greedy Filling Algorithm for Certified Radius:**</span> For a base classifier $f$ and input $x$, find the largest radius $R$ such that for all perturbations $\delta$ with $||\delta||_2 \leq R$, the smoothed classifier $g(x + \delta)$ predicts the same class as $g(x)$.

- $g(x) = \int_{\eta \in B(0, r)} f(x + \eta) p(\eta) d\eta$, where $p(\eta)$ is the probability density function of the Gaussian distribution
- $g(x + \delta) = \int_{\eta \in B(0, r)} f(x + \delta + \eta) p(\eta) d\eta$
- assign class to $B(x, r) \setminus B(x + \delta, r)$ and $B(x + \delta, r) \setminus B(x, r)$ based on majority class in those regions
- for every point $y\in B(x, r) \cap B(x + \delta, r)$, compute likelihood ratio $\dfrac{p(y - x)}{p(y - (x + \delta))} = \exp\left(\dfrac{||\delta||_2^2 + 2\langle y - x, \delta \rangle}{2\sigma^2}\right)$
- sort points in $B(x, r) \cap B(x + \delta, r)$ by likelihood ratio and assign them to classes
