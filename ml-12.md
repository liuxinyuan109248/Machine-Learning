# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 12 Robust Machine Learning</span>

### <span style="color: #6ED3C5">Adversarial Attacks</span>

**Adversarial Training Objective:** $\mathbb{E}_{adv} = \mathbb{E}_{(x,y) \sim D} [\max_{\delta \in S} L(f_\theta(x + \delta), y)]$

<span style="color: #6FA8FF">**Fast Gradient Sign Method (FGSM):**</span> $\delta = \epsilon \cdot \text{sign}(\nabla_x L(f_\theta(x), y))$

<span style="color: #6FA8FF">**Projected Gradient Descent (PGD):**</span> $\delta_{t+1} = P_\Delta(\delta_t + \eta \nabla_x L(f_\theta(x + \delta_t), y))$ where $\Delta = \{\delta : ||\delta||_\infty \leq \epsilon\}$ and $P_\Delta(\delta) = \text{Clip}(\delta, [-\epsilon, \epsilon])$

<span style="color: #6FA8FF">**Danskin's Theorem:**</span> If $g(x) = \max_{z \in Z} f(x, z)$ and $f$ is differentiable, then $\nabla g(x) = \nabla_x f(x, z^*)$ where $z^* = \arg\max_{z \in Z} f(x, z)$. $\implies \nabla_\theta \mathbb{E}_{adv} = \mathbb{E}_{(x,y) \sim D} [\nabla_\theta L(f_\theta(x + \delta^*), y)]$ where $\delta^* = \arg\max_{\delta \in S} L(f_\theta(x + \delta), y)$

<span style="color: #6FA8FF">**Adversarial Training Algorithm:**</span>

1. Sample mini-batch $\{(x_i, y_i)\}_{i=1}^m$ from $D$
2. For each $(x_i, y_i)$, compute adversarial perturbation $\delta_i$ using PGD
   - Initialize $\delta_i^{(0)} = 0$
   - For $t = 0$ to $T-1$: $\delta_i^{(t+1)} = P_\Delta(\delta_i^{(t)} + \eta \nabla_x L(f_\theta(x_i + \delta_i^{(t)}), y_i))$
   - Set $\delta_i = \delta_i^{(T)}$
3. Update model parameters: $\theta \leftarrow \theta - \dfrac{\eta}{m} \nabla_\theta \sum_{i=1}^m L(f_\theta(x_i + \delta_i), y_i)$

**Obfuscated Gradients:** Techniques that make gradient-based attacks less effective without truly improving robustness

- **Shattered Gradients:** Non-differentiable operations that hinder gradient computation
- **Stochastic Gradients:** Randomized defenses that introduce noise in gradient estimation
- **Vanishing/Exploding Gradients:** Numerical issues that lead to unreliable gradient information

**Attack Strategies Against Obfuscated Gradients:**

- <span style="color: #6FA8FF">**Backward Pass Differentiable Approximation (BPDA):**</span> Replace non-differentiable components with differentiable approximations during backpropagation (e.g., replace a quantization step with an identity function)
- <span style="color: #6FA8FF">**Expectation Over Transformation (EOT):**</span> Average gradients over multiple stochastic transformations to obtain a more reliable estimate to perform attacks

### <span style="color: #6ED3C5">Certified Radius and Greedy Filling Algorithm</span>

**Certified Robustness:** Methods that provide formal guarantees on model robustness within a specified perturbation set

<span style="color: #6FA8FF">**Randomized Smoothing:**</span> Given a base classifier $f: \mathbb{R}^d \rightarrow \{1, \ldots, K\}$, the smoothed classifier $g$ is defined as: $g(x) = \arg\max_c \mathbb{P}_{\eta \sim \mathcal{N}(0, \sigma^2 I)}(f(x + \eta) = c)$.

<span style="color: #6FA8FF">**Neyman-Pearson Lemma:**</span>  To test a null hypothesis $H_0: X \sim q(x)$ against an alternative $H_1: X \sim p(x)$, the **most powerful test** at significance level $\alpha$ is defined by the rejection region: $R = \left\{ x : \dfrac{p(x)}{q(x)} > k \right\}$, where the threshold $k$ is chosen such that the probability of a false alarm matches $\alpha$: $P( \text{reject } H_0 \mid H_0 \text{ is true} ) = \int_R q(x) dx = \alpha$.

<span style="color: #6FA8FF">**Greedy Filling Algorithm:**</span> An algorithm to compute the certified radius for randomized smoothing by iteratively assigning points to classes based on likelihood ratios.

<span style="color: #6FA8FF">**Certified Radius for Randomized Smoothing:**</span> For a base classifier $f$ and input $x$, if $g(x) = c_A$ with probability $p_A$ and the second most probable class is $c_B$ with probability $p_B$, then the certified radius $R$ is given by $R = \dfrac{\sigma}{2} \left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right)$, where $\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function.

<span style="color: #6FA8FF">**Proof:**</span> **1. The Neyman-Pearson Lemma and the "Worst-case" Region**

To show $g(x+\delta) = c_A$, we must ensure that under the perturbed distribution, the probability of class $c_A$ remains greater than $c_B$. According to the **Neyman-Pearson Lemma**, given a fixed probability $p_A$ under the null distribution $q = \mathcal{N}(x, \sigma^2 I)$, the probability under the alternative distribution $p = \mathcal{N}(x + \delta, \sigma^2 I)$ is **minimized** when the decision region $S = \{ \eta : f(x+\eta) = c_A \}$ is a **half-space** that is "most distant" from the direction of the shift $\delta$. For Gaussian distributions, the likelihood ratio is
$$L(y) = \dfrac{p(y)}{q(y)} = \exp\left(\dfrac{\|y-x\|^2 - \|y-x-\delta\|^2}{2\sigma^2}\right) = \exp\left(\dfrac{2\langle y-x, \delta \rangle - \|\delta\|_2^2}{2\sigma^2}\right)$$
Thus, the worst-case region $S$ is: $S = \left\{ y \in \mathbb{R}^d : \left\langle y - x, \dfrac{\delta}{\|\delta\|_2} \right\rangle \leq \kappa \right\}$ where $\kappa$ is a constant determined by $p_A$.

**2. Lower Bounding $\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_A)$**

We project the $d$-dimensional Gaussian noise $\eta \sim \mathcal{N}(0, \sigma^2 I)$ onto the 1D line defined by the vector $\delta$. Let $Z \sim \mathcal{N}(0, 1)$ be a standard normal random variable. At the original point $x$, we have $\mathbb{P}(\eta \in S) = p_A \implies \mathbb{P}(\sigma Z \leq \kappa) = p_A$, which implies $\kappa = \sigma \Phi^{-1}(p_A)$. When the input is perturbed by $\delta$, the noise becomes $\eta' \sim \mathcal{N}(\delta, \sigma^2 I)$. In the direction of $\delta$, the random variable effectively becomes $\sigma Z + \|\delta\|_2$.
The probability of $c_A$ is lower-bounded by:
$$\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_A) \geq \mathbb{P}(\sigma Z + \|\delta\|_2 \leq \kappa) = \mathbb{P}\left(Z \leq \Phi^{-1}(p_A) - \frac{\|\delta\|_2}{\sigma}\right) = \Phi\left(\Phi^{-1}(p_A) - \frac{\|\delta\|_2}{\sigma}\right)$$

**3. Upper Bounding $\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_B)$**

To ensure the most difficult case for $c_A$ staying the leader, we assume $c_B$ occupies the remaining probability space in the direction that benefits most from the shift $\delta$. By symmetry: $\mathbb{P}_{\eta}(f(x + \delta + \eta) = c_B) \leq \Phi\left(\Phi^{-1}(p_B) + \dfrac{\|\delta\|_2}{\sigma}\right)$.

**4. Deriving the Certified Radius $R$**

The prediction $g(x + \delta)$ is guaranteed to be $c_A$ if $\Phi\left(\Phi^{-1}(p_A) - \dfrac{\|\delta\|_2}{\sigma}\right) > \Phi\left(\Phi^{-1}(p_B) + \dfrac{\|\delta\|_2}{\sigma}\right)$. Since $\Phi$ is strictly monotonic, this is equivalent to $\|\delta\|_2 < \dfrac{\sigma}{2} \left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right)$. Thus, the certified radius $R$ is: $R = \dfrac{\sigma}{2} \left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right)$.
