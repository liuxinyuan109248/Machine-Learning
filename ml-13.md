# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 13 Hyperparameter Tuning</span>

<span style="color: #6FA8FF">**Hyperparameter tuning**</span> is the process of optimizing the parameters that govern the learning process (e.g., learning rate, batch size) rather than the model weights themselves. Since evaluating a set of hyperparameters is often computationally expensive, we use systematic strategies to navigate the search space efficiently.

### <span style="color: #6ED3C5">Bayesian Optimization</span>

<span style="color: #6FA8FF">**Bayesian Optimization**</span> is a strategy for optimization of black-box functions. It is particularly effective when the objective function is expensive to evaluate. It works by constructing a probabilistic "surrogate model" of the objective function and using an "acquisition function" to decide where to sample next, effectively balancing the trade-off between exploring unknown regions and exploiting known high-performing areas.

1. **Surrogate Model:** Use a probabilistic model (e.g., Gaussian Process) to model the objective function $f(x)$ based on observed data points $\{(x_i, f(x_i))\}$.
   - Mean function: $\mu(x) = \mathbb{E}[f(x)]$
   - Covariance function: $k(x, x') = \mathbb{E}[(f(x) - \mu(x))(f(x') - \mu(x'))]$
2. **Acquisition Function:** Define an acquisition function $a(x)$ that balances exploration and exploitation. Common choices include:
   - **Expected Improvement (EI):** $a_{EI}(x) = \mathbb{E}[\max(0, f(x) - f(x^+))]$
   - **Upper Confidence Bound (UCB):** $a_{UCB}(x) = \mu(x) + \kappa \sigma(x)$ where $\sigma(x)$ is the standard deviation of the surrogate model and $\kappa$ is a tunable parameter.
3. **Optimization Loop:**
   - Initialize with a set of random evaluations $\{(x_i, f(x_i))\}$.
   - Repeat until convergence or budget exhaustion:
     1. Fit the surrogate model to the observed data.
     2. Optimize the acquisition function to select the next point $x_{next} = \arg\max_x a(x)$.
     3. Evaluate the objective function at $x_{next}$ to obtain $f(x_{next})$.
     4. Update the observed data with the new point $(x_{next}, f(x_{next}))$.

### <span style="color: #6ED3C5">Gradient-Based Optimization</span>

When hyperparameters are continuous and the validation loss is differentiable, we can use gradient descent to optimize them. This approach treats the training process itself as a differentiable operation. By using reverse-mode differentiation (backpropagating through the entire training procedure), we can calculate how the final validation loss changes with respect to the initial hyperparameters.

<span style="color: #6FA8FF">**Stochastic Gradient Descent (SGD) with Momentum:**</span>

1. Input: Initial parameters $w_0$, learning rates $\{\alpha_t\}$, decay rates $\{\gamma_t\}$, loss function $L(w, \theta, t)$, and number of iterations $T$.
2. Initialize momentum vector $v_0 = 0$.
3. For $t = 1$ to $T$:
   - Compute gradient: $g_t = \nabla_w L(w_{t-1}, \theta, t)$
   - Update momentum: $v_t = \gamma_t v_{t-1} - (1 - \gamma_t) g_t$
   - Update parameters: $w_t = w_{t-1} + \alpha_t v_t$
4. Return parameters $w_T$.

<span style="color: #6FA8FF">**Reverse-Mode Differentiation (Backpropagation) of SGD:**</span>

1. Input: Final parameters $w_T$, final momentum vector $v_T$, learning rates $\{\alpha_t\}$, decay rates $\{\gamma_t\}$, training loss function $L(w, \theta, t)$, validation loss function $f(w)$, and number of iterations $T$.
2. Initialize adjoint variables (gradients): $dw = \nabla_w f(w_T)$, $dv = 0$, $d\theta = 0$, $d\alpha = 0$, $d\gamma = 0$.
3. For $t = T$ down to $1$:
  $$g_t = \nabla_w L(w_t, \theta, t), \quad v_{t-1} = \dfrac{v_t + (1 - \gamma_t) g_t}{\gamma_t}, \quad w_{t-1} = w_t - \alpha_t v_t$$
  $$d\alpha_t = \left(\dfrac{\partial L}{\partial w_t}\right)^T \dfrac{\partial w_t}{\partial \alpha_t} = dw^T v_t, \quad d\gamma_t = \left(\dfrac{\partial L}{\partial v_t}\right)^T \dfrac{\partial v_t}{\partial \gamma_t} = dv^T (v_{t-1} + g_t)$$
  $$dw = dw_{t-1} = \dfrac{\partial w_t}{\partial w_{t-1}} dw_t + \dfrac{\partial v_t}{\partial w_{t-1}} dv_t = dw - (1 - \gamma_t) \nabla_w^2 L(w_{t-1}, \theta, t) dv$$
  $$d\theta = d\theta + \left( \frac{\partial v_t}{\partial \theta} \right)^T dv_t = d\theta - (1 - \gamma_t) \nabla_\theta \nabla_w L(w_{t-1}, \theta, t) dv$$
  $$dv = dv_{t-1} = \dfrac{\partial w_{t-1}}{\partial v_{t-1}} dw_{t-1} + \dfrac{\partial v_t}{\partial v_{t-1}} dv_t = \alpha_{t-1} dw + \gamma_t dv$$
4. Return gradients $dw, dv, d\theta, d\alpha, d\gamma$.

### <span style="color: #6ED3C5">Multi-Armed Bandit Approaches</span>

These methods treat hyperparameter tuning as a resource allocation problem. Instead of training every configuration to completion, algorithms like <span style="color: #6FA8FF">**Successive Halving**</span> start with many configurations and "prune" the low-performing ones early. This allows the budget to be concentrated on the most promising candidates, speeding up the search compared to standard Grid or Random Search.

<span style="color: #6FA8FF">**Successive Halving Algorithm:**</span>

1. Input: Total budget $B$ and number of configurations $n$.
2. Initialize: Set $B' = \dfrac{B}{\log_2 n}$ and $S_0 = [n]$.
3. For $i = 0$ to $\log_2 n - 1$:
   - Evaluate each configuration in $S_i$ for $\dfrac{B'}{|S_i|}$ resources and obtain performance metrics.
   - Select top half configurations: let $S_{i+1}$ be the top $\dfrac{|S_i|}{2}$ configurations based on performance.
4. Return the best configuration from $S_{\log_2 n}$.
