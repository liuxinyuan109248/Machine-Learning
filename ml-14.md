# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 14 Interpretability</span>

### <span style="color: #6ED3C5">Local Interpretable Model-Agnostic Explanations (LIME)</span>

1. **Input:** A black-box model $f$, an instance $x$ to explain, a number of samples $N$, and a kernel function $\pi_x(z)$ that defines the locality around $x$.
2. **Generate Perturbations:** Create $N$ perturbed samples $\{z_i\}_{i=1}^N$ around $x$ by randomly sampling in the feature space.
3. **Get Predictions:** For each perturbed sample $z_i$, obtain the prediction from the black-box model: $f(z_i)$.
4. **Compute Weights:** Calculate the weights for each perturbed sample using the kernel function: $w_i = \pi_x(z_i)$.
5. **Fit Interpretable Model:** Fit a simple interpretable model $g$ (e.g., linear regression) to the weighted dataset $\{(z_i, f(z_i), w_i)\}$ by minimizing the following loss: $\mathcal{L}(f, g, \pi_x) = \sum_{i=1}^N w_i (f(z_i) - g(z_i))^2 + \Omega(g)$, where $\Omega(g)$ is a regularization term to ensure interpretability.
6. **Output Explanation:** The interpretable model $g$ serves as the local explanation for the prediction of $f$ at instance $x$.

**Fundamental Axioms for Attribution Methods:**

1. <span style="color: #6FA8FF">**Sensitivity:**</span> If two inputs differ in one feature and have different outputs, the feature should be assigned a non-zero attribution.
2. <span style="color: #6FA8FF">**Implementation Invariance:**</span> Two functionally equivalent models (i.e., they produce the same outputs for all inputs) should yield the same attributions for any given input.
3. <span style="color: #6FA8FF">**Completeness:**</span> The sum of the attributions across all features should equal the difference between the output of the model at the input and the output at a baseline input.
4. <span style="color: #6FA8FF">**Linearity:**</span> For models that are linear combinations of other models, the attributions should be the same linear combination of the attributions from the individual models.
5. <span style="color: #6FA8FF">**Symmetry:**</span> If two features contribute equally to the output of the model, they should receive equal attributions.

### <span style="color: #6ED3C5">Integrated Gradients</span>

1. **Input:** A differentiable model $F: \mathbb{R}^n \to \mathbb{R}$, an input instance $x \in \mathbb{R}^n$, a baseline instance $x' \in \mathbb{R}^n$ (e.g., zero vector), and the number of steps $m$ for approximation.
2. **Compute Integrated Gradients:** For each feature $i$, compute the integrated gradient as:
   $$IG_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial F(x' + \alpha (x - x'))}{\partial x_i} d\alpha$$
   This integral can be approximated using a Riemann sum:
   $$IG_i(x) \approx (x_i - x'_i) \times \frac{1}{m} \sum_{k=1}^m \frac{\partial F(x' + \frac{k}{m} (x - x'))}{\partial x_i}$$
   Note: The integrated gradients satisfy the completeness property: $\sum_{i=1}^n IG_i(x) = F(x) - F(x')$.
3. **Output Attribution:** The vector $IG(x) = [IG_1(x), IG_2(x), \ldots, IG_n(x)]$ represents the attribution of each feature to the prediction made by the model $F$ at instance $x$.

### <span style="color: #6ED3C5">Shapley Additive Explanations (SHAP)</span>

1. **Input:** A model $f$, an instance $x$ to explain, a background dataset $D$, and a set of features $F = \{1, 2, \ldots, n\}$.
2. **Compute Shapley Values:** For each feature $i \in F$, compute the Shapley value $\phi_i$ as:
   $$\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|! (n - |S| - 1)!}{n!} \left[ f_{S \cup \{i\}}(x) - f_S(x) \right]$$
   where $f_S(x)$ is the expected model output when only the features in subset $S$ are known (i.e., other features are marginalized using the background dataset $D$).
