# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 14 Interpretability</span>

<span style="color: #6FA8FF">**Model interpretability**</span> aims to bridge the gap between model performance and human understanding. As models become more complex (the "black-box" problem), we require methods to explain why a specific prediction was made, ensuring safety, fairness, and trust in deployed systems.

### <span style="color: #6ED3C5">Local Interpretable Model-Agnostic Explanations (LIME)</span>

<span style="color: #6FA8FF">**LIME**</span> is a post-hoc, model-agnostic method that explains individual predictions by approximating a complex black-box model locally with a simpler, inherently interpretable model (like a linear regressor or decision tree). It works on the assumption that while a model may be globally complex, it is often linear or simple in the immediate neighborhood of a single data point.

1. **Input:** A black-box model $f$, an instance $x$ to explain, a number of samples $N$, and a kernel function $\pi_x(z)$ that defines the locality around $x$.
2. **Generate Perturbations:** Create $N$ perturbed samples $\{z_i\}_{i=1}^N$ around $x$ by randomly sampling in the feature space.
3. **Get Predictions:** For each perturbed sample $z_i$, obtain the prediction from the black-box model: $f(z_i)$.
4. **Compute Weights:** Calculate the weights for each perturbed sample using the kernel function: $w_i = \pi_x(z_i)$.
5. **Fit Interpretable Model:** Fit a simple interpretable model $g$ (e.g., linear regression) to the weighted dataset $\{(z_i, f(z_i), w_i)\}$ by minimizing the following loss: $\mathcal{L}(f, g, \pi_x) = \sum_{i=1}^N w_i (f(z_i) - g(z_i))^2 + \Omega(g)$, where $\Omega(g)$ is a regularization term to ensure interpretability.
6. **Output Explanation:** The interpretable model $g$ serves as the local explanation for the prediction of $f$ at instance $x$.

### <span style="color: #6ED3C5">Fundamental Axioms for Attribution Methods</span>

Attribution methods aim to assign importance scores to input features based on their contribution to a model's prediction. To ensure that these attributions are meaningful and consistent, several fundamental axioms have been proposed:

1. <span style="color: #6FA8FF">**Sensitivity:**</span> If two inputs differ in one feature and have different outputs, the feature should be assigned a non-zero attribution.
2. <span style="color: #6FA8FF">**Implementation Invariance:**</span> Two functionally equivalent models (i.e., they produce the same outputs for all inputs) should yield the same attributions for any given input.
3. <span style="color: #6FA8FF">**Completeness:**</span> The sum of the attributions across all features should equal the difference between the output of the model at the input and the output at a baseline input.
4. <span style="color: #6FA8FF">**Linearity:**</span> For models that are linear combinations of other models, the attributions should be the same linear combination of the attributions from the individual models.
5. <span style="color: #6FA8FF">**Symmetry:**</span> If two features contribute equally to the output of the model, they should receive equal attributions.

### <span style="color: #6ED3C5">Integrated Gradients</span>

<span style="color: #6FA8FF">**Integrated Gradients (IG)**</span> is an axiomatic attribution method designed for differentiable models like neural networks. Standard gradient-based explanations often suffer from "saturation," where the gradient becomes zero even if a feature is important. IG addresses this by integrating the gradients along a linear path from a "baseline" (neutral) input to the actual input, ensuring that the total attribution accounts for the entire change in the model's output.

1. **Input:** A differentiable model $F: \mathbb{R}^n \to \mathbb{R}$, an input instance $x \in \mathbb{R}^n$, a baseline instance $x' \in \mathbb{R}^n$ (e.g., zero vector), and the number of steps $m$ for approximation.
2. **Compute Integrated Gradients:** For each feature $i$, compute the integrated gradient as:
   $$IG_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial F(x' + \alpha (x - x'))}{\partial x_i} d\alpha$$
   This integral can be approximated using a Riemann sum:
   $$IG_i(x) \approx (x_i - x'_i) \times \frac{1}{m} \sum_{k=1}^m \frac{\partial F\left(x' + \dfrac{k}{m} (x - x')\right)}{\partial x_i}$$
   Note: The integrated gradients satisfy the completeness property: $\sum_{i=1}^n IG_i(x) = F(x) - F(x')$.
3. **Output Attribution:** The vector $IG(x) = [IG_1(x), IG_2(x), \ldots, IG_n(x)]$ represents the attribution of each feature to the prediction made by the model $F$ at instance $x$.

### <span style="color: #6ED3C5">Shapley Additive Explanations (SHAP) (Skip for Final)</span>

1. **Input:** A model $f$, an instance $x$ to explain, a background dataset $D$, and a set of features $F = \{1, 2, \ldots, n\}$.
2. **Compute Shapley Values:** For each feature $i \in F$, compute the Shapley value $\phi_i$ as:
   $$\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|! (n - |S| - 1)!}{n!} \left[ f_{S \cup \{i\}}(x) - f_S(x) \right]$$
   where $f_S(x)$ is the expected model output when only the features in subset $S$ are known (i.e., other features are marginalized using the background dataset $D$).
