# <span style="color: #4FC3F7">Machine Learning</span>

## <span style="color: #F2A07B">Lec 01 History of AI

### <span style="color: #6ED3C5">History of AI</span>

- 1950, Turing Test
- 1951, SNARC by Marvin Minsky
- 1956, Dartmouth Conference, John McCarthy coins term "Artificial Intelligence"
- 1974-1980, first AI winter
- 1982, Hopfield Network
- 1986, backpropagation by David Rumelhart, Geoffrey Hinton and Ronald Williams
- 1987-1993, second AI winter
- 1997, IBM's Deep Blue beats Garry Kasparov
- 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton wins ImageNet competition
- 2016, AlphaGo by DeepMind beats Lee Sedol
- 2018, foundation models (e.g., GPT-3, BERT)

## <span style="color: #F2A07B">Lec 02 Unsupervised Learning and Gradient Descent</span>

<span style="color: #6FA8FF">**Unsupervised Learning:</span>**

- Clustering
- Principal Component Analysis (PCA)
  - find the directions of maximum variance in high-dimensional data
  - the best-fit line minimizes the sum of squared distances from the points to the line
- Generative Models
- Anomaly Detection
- Dimensionality Reduction

<span style="color: #6FA8FF">**Convexity:**</span> If $f$ is twice continuously differentiable, then the following statements are equivalent:

- $f(tx+(1-t)y)\le tf(x)+(1-t)f(y),\forall t\in[0,1],x,y$,
- $f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle,\forall x,y$,
- gradient is non-decreasing,
- Hessian is positive semi-definite.

<span style="color: #6FA8FF">**$L$-Lipschitz Continuity:**</span> $|f(y)-f(x)|\le L\|y-x\|,\forall x,y$

<span style="color: #6FA8FF">**$L$-Smoothness / Gradient Lipschitz Continuity:**</span> If $f$ is twice continuously differentiable, then the following statements are equivalent:

- **(a):** $|\nabla f(y)-\nabla f(x)|\le L\|y-x\|,\forall x,y$,
- **(b):** $|f(y)-f(x)-\langle\nabla f(x),y-x\rangle|\le\dfrac{L}{2}\|y-x\|^2,\forall x,y$,
- **(c):** $-L\le\lambda_{\min}(\nabla^2f(x))\le\lambda_{\max}(\nabla^2f(x))\le L$, or equivalently, $-LI\preceq\nabla^2f(x)\preceq LI,\forall x$.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

- **(a)$\Rightarrow$(b):**
  $$f(y)-f(x)=\int_0^1\langle\nabla f(x+t(y-x)),y-x\rangle dt=\langle\nabla f(x),y-x\rangle+\int_0^1\langle\nabla f(x+t(y-x))-\nabla f(x),y-x\rangle dt$$
  $$
  \begin{aligned}
  \implies|f(y)-f(x)-\langle\nabla f(x),y-x\rangle|\le&\int_0^1|\langle\nabla f(x+t(y-x))-\nabla f(x),y-x\rangle| dt \\
  \le&\int_0^1L\|t(y-x)\|\cdot\|y-x\| dt=\dfrac{L}{2}\|y-x\|^2 \\
  \end{aligned}
  $$
- **(b)$\Rightarrow$(c):**
  $$|\langle\nabla f(x)-\nabla f(y),x-y\rangle|=|f(y)-f(x)-\langle\nabla f(x),y-x\rangle|+|f(x)-f(y)-\langle\nabla f(y),x-y\rangle|\le L\|x-y\|^2$$
  $$\implies\left|\left\langle\frac{\nabla f(x)-\nabla f(y)}{\|x-y\|},\frac{x-y}{\|x-y\|}\right\rangle\right|\le L\implies -L\le\lambda_{\min}(\nabla^2f(x))\le\lambda_{\max}(\nabla^2f(x))\le L$$
  (otherwise, let $y=x+\epsilon v$, where $v$ is the eigenvector corresponding to $\lambda_{\max}(\nabla^2f(x))$ or $\lambda_{\min}(\nabla^2f(x))$, and let $\epsilon\to 0$)
- **(c)$\Rightarrow$(a):** $\|\nabla f(y)-\nabla f(x)\|=\left\|\int_0^1\nabla^2f(x+t(y-x))(y-x) dt\right\|\le\int_0^1\|\nabla^2f(x+t(y-x))\|\cdot\|y-x\| dt\le L\|y-x\|$

</details>

<span style="color: #6FA8FF">**Proposition:**</span> If $f$ is $L$-smooth, then for $y=x-\eta\nabla f(x)$ and $\eta<\dfrac{2}{L}$, we have $f(y)\le f(x)$.

<span style="color: #6FA8FF">**Proof:**</span> $f(y)-f(x)\le\langle\nabla f(x),y-x\rangle+\dfrac{L}{2}\|y-x\|^2=\langle\nabla f(x),-\eta\nabla f(x)\rangle+\dfrac{L}{2}\|-\eta\nabla f(x)\|^2=\eta\left(\dfrac{L\eta}{2}-1\right)\|\nabla f(x)\|^2\le 0$

<span style="color: #6FA8FF">**Proposition:**</span> If $f$ is $L$-smooth, then for $y=x-\dfrac{1}{L}\nabla f(x)$, we have $f(y)\le f(x)-\dfrac{1}{2L}\|\nabla f(x)\|^2$. (proof omitted)

<span style="color: #6FA8FF">**Proposition:**</span> If $f$ is twice continuously differentiable, then the following statements are equivalent:

- **(a):** $\alpha\le\lambda_{\min}(\nabla^2f(x))\le\lambda_{\max}(\nabla^2f(x))\le\beta$, or equivalently, $\alpha I\preceq\nabla^2f(x)\preceq\beta I,\forall x$,
- **(b):** $\alpha\|x-y\|^2\le\langle\nabla f(x)-\nabla f(y),x-y\rangle\le\beta\|x-y\|^2,\forall x,y$.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

- **(a)$\Rightarrow$(b):**
  $$
  \begin{aligned}
  &\langle\nabla f(x)-\nabla f(y),x-y\rangle=\left\langle\int_0^1\nabla^2f(x+t(y-x))(y-x)dt,y-x\right\rangle\\
  =&\int_0^1\langle\nabla^2f(x+t(y-x))(y-x)dt,y-x\rangle\in[\alpha\|x-y\|^2,\beta\|x-y\|^2]\\
  \end{aligned}
  $$
- **(b)$\Rightarrow$(a):** For any unit vector $v$, let $y=x+\epsilon v$, then
  $$\alpha\epsilon^2\le\langle\nabla f(x+\epsilon v)-\nabla f(x),\epsilon v\rangle\le\beta\epsilon^2\implies\alpha\le\left\langle\frac{\nabla f(x+\epsilon v)-\nabla f(x)}{\epsilon},v\right\rangle\le\beta$$
  Let $\epsilon\to 0$, we have $\alpha\le v^T\nabla^2f(x)v\le\beta$. Since $v$ is arbitrary, $\alpha I\preceq\nabla^2f(x)\preceq\beta I$.

</details>

<span style="color: #6FA8FF">**Theorem:**</span> If $f$ is twice continuously differentiable, then the following statements are equivalent:

- **(a):** $0\le f(y)-f(x)-\langle\nabla f(x),y-x\rangle\le\dfrac{L}{2}\|x-y\|^2,\forall x,y$,
- **(b):** $f(y)-f(x)-\langle\nabla f(x),y-x\rangle\ge\dfrac{1}{2L}\|\nabla f(x)-\nabla f(y)\|^2,\forall x,y$,
- **(c):** $0\le\langle\nabla f(x)-\nabla f(y),x-y\rangle\le L\|x-y\|^2,\forall x,y$,
- **(d):** $\langle\nabla f(x)-\nabla f(y),x-y\rangle\ge\dfrac{1}{L}\|\nabla f(x)-\nabla f(y)\|^2,\forall x,y$,
- **(e):** $0\le\lambda_{\min}(\nabla^2f(x))\le\lambda_{\max}(\nabla^2f(x))\le L$, or equivalently, $0\preceq\nabla^2f(x)\preceq LI,\forall x$,
- **(f):** $f$ is convex and $L$-smooth.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

- **(a)$\Leftrightarrow$(f):** by the definition of convexity and $L$-smoothness.
- **(c)$\Leftrightarrow$(e):** apply the previous proposition with $\alpha=0$ and $\beta=L$.
- **(e)$\Leftrightarrow$(f):** by the definition of convexity and $L$-smoothness.
- **(f)$\Rightarrow$(b):** Let $g(x)=f(x)-\langle\nabla f(y),x\rangle,\forall x$, then $g$ is also convex and $L$-smooth. Since $\nabla g(x)=\nabla f(x)-\nabla f(y)$ and $\nabla g(y)=\nabla f(y)-\nabla f(y)=0$, $y$ is a minimizer of $g$. Therefore, since $g$ is $L$-smooth,
  $$g(y)\le g\left(x-\frac{1}{L}\nabla g(x)\right)\le g(x)-\frac{1}{2L}\|\nabla g(x)\|^2\implies f(y)-\langle\nabla f(y),y\rangle\le f(x)-\langle\nabla f(y),x\rangle-\frac{1}{2L}\|\nabla f(x)-\nabla f(y)\|^2$$
  $$\implies\frac{1}{2L}\|\nabla f(x)-\nabla f(y)\|^2\le f(x)-f(y)-\langle\nabla f(y),x-y\rangle$$
  Swapping $x$ and $y$ gives (b).
- **(b)$\Rightarrow$(d):** Swapping $x$ and $y$ and adding the two inequalities gives (d).
- **(d)$\Rightarrow$(c):**
  $$\langle\nabla f(x)-\nabla f(y),x-y\rangle\ge\frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^2\ge 0;$$
  $$
  \begin{aligned}
  &L\|x-y\|^2=L\|x-y\|^2+\frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^2-\frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^2\\
  \ge&2\langle\nabla f(x)-\nabla f(y),x-y\rangle-\frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^2\ge\langle\nabla f(x)-\nabla f(y),x-y\rangle\\
  \end{aligned}
  $$

</details>

<span style="color: #6FA8FF">**$\mu$-Strongly Convexity:**</span> If $f$ is twice continuously differentiable, then the following statements are equivalent:

- **(a):** $f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle+\dfrac{\mu}{2}\|y-x\|^2,\forall x,y$,
- **(b):** $\langle\nabla f(x)-\nabla f(y),x-y\rangle\ge\mu\|x-y\|^2,\forall x,y$,
- **(c):** $\lambda_{\min}(\nabla^2f(x))\ge\mu$, or equivalently, $\nabla^2f(x)\succeq\mu I,\forall x$,
- **(d):** $f(\alpha x+(1-\alpha)y)\le\alpha f(x)+(1-\alpha)f(y)-\dfrac{\mu}{2}\alpha(1-\alpha)\|x-y\|^2,\forall x,y,\alpha\in[0,1]$.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

- **(a)$\Rightarrow$(b):** Swapping $x$ and $y$ in (a) and adding the two inequalities gives (b).
- **(b)$\Rightarrow$(c):** Apply the previous proposition with $\alpha=\mu$.
- **(c)$\Rightarrow$(a):** Let $g(x)=f(x)-\dfrac{\mu}{2}\|x\|^2$, then $\nabla g(x)=\nabla f(x)-\mu x$ and $\nabla^2g(x)=\nabla^2f(x)-\mu I\succeq 0$, therefore $g$ is convex. By the definition of convexity,
  $$g(y)\ge g(x)+\langle\nabla g(x),y-x\rangle\implies f(y)-\frac{\mu}{2}\|y\|^2\ge f(x)-\frac{\mu}{2}\|x\|^2+\langle\nabla f(x)-\mu x,y-x\rangle$$
  $$\implies f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}(\|y\|^2-\|x\|^2)+\mu\langle x,y-x\rangle=f(x)+\langle\nabla f(x),y-x\rangle+\frac{\mu}{2}\|y-x\|^2$$
- **(a)$\Rightarrow$(d):** Let $g(\alpha)=f(\alpha x+(1-\alpha)y)-\alpha f(x)-(1-\alpha)f(y)+\dfrac{\mu}{2}\alpha(1-\alpha)\|x-y\|^2$, then $g(0)=g(1)=0$ and $g(\alpha)\le 0,\forall\alpha\in[0,1]$. Therefore $g'(1)\ge 0\implies\langle\nabla f(x),x-y\rangle-f(x)+f(y)-\dfrac{\mu}{2}\|x-y\|^2\ge 0$. Rewriting gives (d).
- **(d)$\Rightarrow$(a):**
  $$
  \begin{aligned}
  f(y)&\ge f(\alpha x+(1-\alpha)y)+\langle\nabla f(\alpha x+(1-\alpha)y),y-(\alpha x+(1-\alpha)y)\rangle+\frac{\mu}{2}\|\alpha x+(1-\alpha)y-y\|^2\\
  &=f(\alpha x+(1-\alpha)y)+\alpha\langle\nabla f(\alpha x+(1-\alpha)y),y-x\rangle+\frac{\mu}{2}\alpha^2\|x-y\|^2\\
  \end{aligned}
  $$
  $$
  \begin{aligned}
  f(x)&\ge f(\alpha x+(1-\alpha)y)+\langle\nabla f(\alpha x+(1-\alpha)y),x-(\alpha x+(1-\alpha)y)\rangle+\frac{\mu}{2}\|\alpha x+(1-\alpha)y-x\|^2\\
  &=f(\alpha x+(1-\alpha)y)+(1-\alpha)\langle\nabla f(\alpha x+(1-\alpha)y),x-y\rangle+\frac{\mu}{2}(1-\alpha)^2\|x-y\|^2\\
  \end{aligned}
  $$
  $$\implies\alpha f(x)+(1-\alpha)f(y)\ge f(\alpha x+(1-\alpha)y)+\frac{\mu}{2}\alpha(1-\alpha)\|x-y\|^2$$
  Rearranging gives (a).

</details>

<span style="color: #6FA8FF">**Proposition:**</span> If $f$ is $\mu$-strongly convex, then $f$ is $\mu$-PL, i.e., $\dfrac{1}{2\mu}\|\nabla f(x)\|^2\ge f(x)-f(x^*),\forall x$, where $x^*=\arg\min f(x)$.

<span style="color: #6FA8FF">**Proof:**</span> $f(x^*)\ge f(x)+\langle\nabla f(x),x^*-x\rangle+\dfrac{\mu}{2}\|x^*-x\|^2\ge f(x)-\|\nabla f(x)\|\cdot\|x^*-x\|+\dfrac{\mu}{2}\|x^*-x\|^2\ge f(x)-\dfrac{1}{2\mu}\|\nabla f(x)\|^2$

### <span style="color: #6ED3C5">Convex Function Convergence</span>

<span style="color: #6FA8FF">**Theorem:**</span> If $f$ is convex and $L$-smooth and $x^*=\arg\min f(x)$, then running gradient descent with step size $\eta\le\dfrac{1}{L}$ gives $f(x_t)-f(x^*)\le\dfrac{\|x_0-x^*\|^2}{2\eta t}$, and therefore $T=\dfrac{L\|x_0-x^*\|^2}{2\eta L\epsilon}$ iterations are sufficient to achieve $f(x_T)-f(x^*)\le\epsilon$.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

Since $f$ is $L$-smooth, $f(x_{i+1})\le f(x_i)-\eta\left(1-\dfrac{L\eta}{2}\right)\|\nabla f(x_i)\|^2\le f(x_i)-\dfrac{\eta}{2}\|\nabla f(x_i)\|^2$. By convexity, $f(x_i)\le f(x^*)+\langle\nabla f(x_i),x_i-x^*\rangle$.
Combining the two inequalities,
$$
\begin{aligned}
f(x_{i+1})&\le f(x_i)-\frac{\eta}{2}\|\nabla f(x_i)\|^2\le f(x^*)+\langle\nabla f(x_i),x_i-x^*\rangle-\frac{\eta}{2}\|\nabla f(x_i)\|^2 \\
&\le f(x^*)-\frac{1}{\eta}\langle x_{i+1}-x_i,x_i-x^*\rangle-\frac{1}{2\eta}\|x_{i+1}-x_i\|^2=f(x^*)+\frac{1}{2\eta}\|x_i-x^*\|^2-\frac{1}{2\eta}\|x_{i+1}-x^*\|^2 \\
\end{aligned}
$$
Take telescoping sum over $i=0,1,\ldots,t-1$, $\sum_{i=0}^{t-1}(f(x_{i+1})-f(x^*))\le\dfrac{1}{2\eta}(\|x_0-x^*\|^2-\|x_t-x^*\|^2)\le\dfrac{\|x_0-x^*\|^2}{2\eta}$. Since $f(x_i)$ is non-increasing, $f(x_t)-f(x^*)\le\dfrac{1}{t}\sum_{i=0}^{t-1}(f(x_{i+1})-f(x^*))\le\dfrac{\|x_0-x^*\|^2}{2\eta t}\implies f(x_T)-f(x^*)\le\epsilon$.

</details>

<span style="color: #6FA8FF">**Theorem:**</span> If $f$ is $\mu$-strongly convex and $L$-smooth and $x^*=\arg\min f(x)$, then running gradient descent with step size $\eta=\dfrac{1}{L}$ gives $f(x_t)-f(x^*)\le\left(1-\dfrac{\mu}{L}\right)^t(f(x_0)-f(x^*))$.

<span style="color: #6FA8FF">**Proof:**</span> $\dfrac{\mu}{L}(f(x_t)-f(x^*))\le\dfrac{1}{2L}\|\nabla f(x_t)\|^2\le f(x_t)-f(x_{t+1})\implies f(x_{t+1})-f(x^*)\le\left(1-\dfrac{\mu}{L}\right)(f(x_t)-f(x^*))$

<span style="color: #6FA8FF">**Proposition:**</span> If $f$ is $\mu$-strongly convex and $L$-smooth and $x^*=\arg\min f(x)$, then running gradient descent with step size $\eta=\dfrac{1}{L}$ gives $\|x_t-x^*\|^2\le\left(1-\dfrac{\mu}{L}\right)^t\|x_0-x^*\|^2$.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

$$\frac{1}{2L}\|\nabla f(x_t)\|^2\le f(x_t)-f(x_{t+1})\le f(x_t)-f(x^*)\le\langle\nabla f(x_t),x_t-x^*\rangle-\frac{\mu}{2}\|x_t-x^*\|^2$$
$$\implies\frac{L}{2}\|x_t-x_{t+1}\|^2\le L\langle x_t-x_{t+1},x_t-x^*\rangle-\frac{\mu}{2}\|x_t-x^*\|^2\implies\|x_{t+1}-x^*\|^2\le\left(1-\frac{\mu}{L}\right)\|x_t-x^*\|^2$$

</details>

<span style="color: #6FA8FF">**Limitations of Gradient Descent:</span>**

- computing full gradient is expensive for large datasets
- could get stuck in saddle points

<span style="color: #6FA8FF">**Stochastic Gradient Descent (SGD):</span>**

- randomly sample a mini-batch of data to compute an unbiased estimate of the gradient
- update parameters using the estimated gradient
- $x_{t+1}=x_t-\eta g_t$, where $g_t$ is the estimated gradient at iteration $t$, $\mathbb{E}[g_t]=\nabla f(x_t)$
- the mini-batch size $|S|$ is usually 64, 128, 256, etc.
- help escape saddle points due to noise in the gradient estimate
- help get the right mini-batch statistics for batch normalization

## <span style="color: #F2A07B">Lec 03 SGD, SVRG and Mirror Descent</span>

### <span style="color: #6ED3C5">Stochastic Gradient Descent (SGD)</span>

<span style="color: #6FA8FF">**Theorem:**</span> If $f$ is convex and $L$-smooth and $x^*=\arg\min f(x)$, then running SGD with variance $Var(g_t)\le\sigma^2$ with step size $\eta\le\dfrac{1}{L}$ gives $\mathbb{E}[f(\overline{x_t})]\le f(x^*)+\dfrac{\|x_0-x^*\|^2}{2\eta t}+\eta\sigma^2$ where $\overline{x_t}=\dfrac{1}{t}\sum_{i=1}^tx_i$.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>
  
First we prove that $\mathbb{E}[f(x_{t+1})]\le f(x^*)+\dfrac{1}{2\eta}\cdot\mathbb{E}[\|x_t-x^*\|^2-\|x_{t+1}-x^*\|^2]+\eta\sigma^2$. Since $f$ is $L$-smooth,
$$
\begin{aligned}
&f(x_{t+1})-f(x^*)-\frac{1}{2\eta}(\|x_t-x^*\|^2-\|x_{t+1}-x^*\|^2) \\
\le&f(x_t)+\langle\nabla f(x_t),x_{t+1}-x_t\rangle+\frac{L}{2}\|x_{t+1}-x_t\|^2-f(x^*)-\frac{1}{2\eta}(\|x_t-x^*\|^2-\|x_t-x^*-\eta g_t\|^2) \\
\le&\langle\nabla f(x_t),x_t-x^*\rangle+\langle\nabla f(x_t),x_{t+1}-x_t\rangle+\frac{L\eta^2}{2}\|g_t\|^2-\langle g_t,x_t-x^*\rangle+\frac{\eta}{2}\|g_t\|^2 \\
=&\langle\nabla f(x_t),x_t-x^*\rangle-\eta\langle\nabla f(x_t),g_t\rangle+\frac{\eta}{2}(1+L\eta)\|g_t\|^2-\langle g_t,x_t-x^*\rangle \\
\end{aligned}
$$
Taking expectation w.r.t. $g_t$ on both sides,
$$
\begin{aligned}
&\mathbb{E}[f(x_{t+1})]-f(x^*)-\frac{1}{2\eta}\cdot\mathbb{E}[\|x_t-x^*\|^2-\|x_{t+1}-x^*\|^2] \\
\le&\langle\nabla f(x_t),x_t-x^*\rangle-\eta\langle\nabla f(x_t),\nabla f(x_t)\rangle+\frac{\eta}{2}(1+L\eta)(\|\nabla f(x_t)\|^2+\sigma^2)-\langle\nabla f(x_t),x_t-x^*\rangle \\
=&\frac{\eta}{2}(1+L\eta)\sigma^2-\frac{\eta}{2}(1-L\eta)\|\nabla f(x_t)\|^2\le\eta\sigma^2, \\
\end{aligned}
$$
finishing the proof of the first part. Now take telescoping sum over $t=0,1,\ldots,t-1$,
$$\sum_{i=0}^{t-1}(\mathbb{E}[f(x_{i+1})]-f(x^*))\le\frac{1}{2\eta}(\|x_0-x^*\|^2-\mathbb{E}[\|x_t-x^*\|^2])+\eta t\sigma^2\le\frac{\|x_0-x^*\|^2}{2\eta}+\eta t\sigma^2$$
By convexity of $f$, $\mathbb{E}[f(\overline{x_t})]\le\dfrac{1}{t}\sum_{i=0}^{t-1}\mathbb{E}[f(x_{i+1})]\le f(x^*)+\dfrac{\|x_0-x^*\|^2}{2\eta t}+\eta\sigma^2$.
  
</details>

<span style="color: #6FA8FF">**Proposition:**</span> SGD achieves an $\epsilon$-optimal solution in $O(1/\epsilon^2)$ iterations, or equivalently, the convergence rate is $O(1/\sqrt{t})$.

<span style="color: #6FA8FF">**Proof:**</span> If we set $t=\dfrac{2\sigma^2\|x_0-x^*\|^2}{\epsilon^2},\eta=\dfrac{\epsilon}{2\sigma^2}$, then $\mathbb{E}[f(\overline{x_t})]-f(x^*)\le\dfrac{\|x_0-x^*\|^2}{2\eta t}+\eta\sigma^2=\epsilon$.

### <span style="color: #6ED3C5">Stochastic Variance Reduced Gradient (SVRG)</span>

- for $s=1,2,\ldots,S$
  - set $\tilde{x}=\tilde{x}_{s-1}$
  - compute full gradient $\tilde{\mu}=\nabla f(\tilde{x})=\dfrac{1}{n}\sum_{i=1}^n\nabla f_i(\tilde{x})$
  - set $x_0=\tilde{x}$
  - for $t=1,2,\ldots,m$
    - randomly pick $i_t\in\{1,2,\ldots,n\}$
    - compute $g_t=\nabla f_{i_t}(x_{t-1})-\nabla f_{i_t}(\tilde{x})+\tilde{\mu}$
    - update $x_t=x_{t-1}-\eta g_t$
  - randomly pick $\tilde{x}_s$ from $\{x_0,x_1,\ldots,x_{m-1}\}$

<span style="color: #6FA8FF">**Theorem:**</span> If each $f_i$ is $\mu$-strongly convex and $L$-smooth, then SVRG achieves linear convergence rate $\dfrac{2L\eta}{1-2L\eta}+\dfrac{1}{m\mu\eta(1-2L\eta)}$, and is faster than GD if the condition number $\kappa=\dfrac{L}{\mu}$ is large.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>

$$
\begin{aligned}
&\mathbb{E}[\|\nabla f_{i_t}(x_{t-1})-\nabla f_{i_t}(\tilde{x})+\tilde{\mu}\|^2] \\
\le&2\mathbb{E}[\|\nabla f_{i_t}(x_{t-1})-\nabla f_{i_t}(x^*)\|^2]+2\mathbb{E}[\|\nabla f_{i_t}(\tilde{x})-\nabla f_{i_t}(x^*)-\nabla f(\tilde{x})\|^2] \\
=&2\mathbb{E}[\|\nabla f_{i_t}(x_{t-1})-\nabla f_{i_t}(x^*)\|^2]+2\mathbb{E}[\|\nabla f_{i_t}(\tilde{x})-\nabla f_{i_t}(x^*)-\mathbb{E}[\nabla f_{i_t}(\tilde{x})-\nabla f_{i_t}(x^*)]\|^2] \\
\le&2\mathbb{E}[\|\nabla f_{i_t}(x_{t-1})-\nabla f_{i_t}(x^*)\|^2]+2\mathbb{E}[\|\nabla f_{i_t}(\tilde{x})-\nabla f_{i_t}(x^*)\|^2] \\
\le&4L\mathbb{E}[f_{i_t}(x_{t-1})-f_{i_t}(x^*)-\langle\nabla f_{i_t}(x^*),x_{t-1}-x^*\rangle]+4L\mathbb{E}[f_{i_t}(\tilde{x})-f_{i_t}(x^*)-\langle\nabla f_{i_t}(x^*),\tilde{x}-x^*\rangle] \\
=&4L(f(x_{t-1})-f(x^*)-\langle\nabla f(x^*),x_{t-1}-x^*\rangle)+4L(f(\tilde{x})-f(x^*)-\langle\nabla f(x^*),\tilde{x}-x^*\rangle) \\
=&4L(f(x_{t-1})-f(x^*)+f(\tilde{x})-f(x^*)) \\
\end{aligned}
$$
and therefore
$$
\begin{aligned}
\mathbb{E}[\|x_t-x^*\|^2]=&\|x_{t-1}-x^*\|^2-2\eta\langle x_{t-1}-x^*,\mathbb{E}[g_t]\rangle+\eta^2\cdot\mathbb{E}[\|g_t\|^2] \\
\le&\|x_{t-1}-x^*\|^2-2\eta\langle x_{t-1}-x^*,\nabla f(x_{t-1})\rangle+4L\eta^2(f(x_{t-1})-f(x^*)+f(\tilde{x})-f(x^*)) \\
\le&\|x_{t-1}-x^*\|^2-2\eta(f(x_{t-1})-f(x^*))+4L\eta^2(f(x_{t-1})-f(x^*)+f(\tilde{x})-f(x^*)) \\
=&\|x_{t-1}-x^*\|^2-2\eta(1-2L\eta)(f(x_{t-1})-f(x^*))+4L\eta^2(f(\tilde{x})-f(x^*)) \\
\end{aligned}
$$
If $\tilde{x}_s$ is randomly picked from $\{x_0,x_1,\ldots,x_{m-1}\}$, then
$$
\begin{aligned}
&\mathbb{E}[\|x_m-x^*\|^2]+2\eta(1-2L\eta)\cdot m\cdot\mathbb{E}[f(\tilde{x}_s)-f(x^*)] \\
\le&\mathbb{E}[\|x_m-x^*\|^2]+2\eta(1-2L\eta)\cdot\sum_{t=1}^m\mathbb{E}[f(x_{t-1})-f(x^*)] \\
\le&\|x_0-x^*\|^2+4mL\eta^2(f(\tilde{x})-f(x^*)) \\
=&\|\tilde{x}-x^*\|^2+4mL\eta^2(f(\tilde{x})-f(x^*)) \\
\le&\frac{2}{\mu}[f(\tilde{x})-f(x^*)]+4mL\eta^2(f(\tilde{x})-f(x^*)) \\
=&2\left(2mL\eta^2+\frac{1}{\mu}\right)(f(\tilde{x})-f(x^*)), \\
\end{aligned}
$$
and therefore $\mathbb{E}[f(\tilde{x}_s)-f(x^*)]\le\left(\dfrac{2L\eta}{1-2L\eta}+\dfrac{1}{m\mu\eta(1-2L\eta)}\right)(f(\tilde{x}_{s-1})-f(x^*))$.

<span style="color: #6FA8FF">**Gradient Descent:**</span> When $\kappa=\dfrac{L}{\mu}$ is large, $\kappa\log(1/\epsilon)$ iterations are needed to achieve an $\epsilon$-optimal solution.

<span style="color: #6FA8FF">**SVRG:**</span> Set $\eta=\dfrac{1}{10L},m=50\kappa$, then $\dfrac{2L\eta}{1-2L\eta}+\dfrac{1}{m\mu\eta(1-2L\eta)}=\dfrac{0.2}{0.8}+\dfrac{10L}{50\kappa\mu\times 0.8}=\dfrac{1}{4}\left(1+\dfrac{L}{\kappa\mu}\right)=\dfrac{1}{2}$. Therefore, $\log(1/\epsilon)$ iterations are needed to achieve an $\epsilon$-optimal solution.

</details>

### <span style="color: #6ED3C5">Mirror Descent</span>

<span style="color: #6FA8FF">**Bregman Divergence:</span>** $V_x(y)=w(y)-w(x)-\langle\nabla w(x),y-x\rangle,\forall x,y$

<span style="color: #6FA8FF">**Proposition:**</span>

- If $w$ is 1-strongly convex, then $V_x(y)\ge\dfrac{1}{2}\|x-y\|^2$
- $\nabla V_x(y)=\nabla w(y)-\nabla w(x)$
- $\langle\nabla V_x(y),y-u\rangle=V_y(u)+V_x(y)-V_x(u)$ (proof omitted)
  
<span style="color: #6FA8FF">**Mirror Descent:**</span> $x_{k+1}=\text{Mirr}(\alpha\cdot\nabla f(x_k)),\text{Mirr}(g)=\arg\min_y(V_x(y)+\langle g,y-x\rangle)$

<span style="color: #6FA8FF">**Theorem:**</span> If $f$ is convex and $\rho$-Lipschitz continuous, then mirror descent achieves an $\epsilon$-optimal solution in $O(\rho^2/\epsilon^2)$ iterations.

<details>
  <summary><b><font color="#6FA8FF">Proof:</font></b> (Click to expand)</summary>
  
Taking derivative w.r.t. $y$ and setting it to zero, $\nabla V_{x_k}(x_{k+1})+\alpha\nabla f(x_k)=0$, and therefore
$$
\begin{aligned}
&\alpha\langle\nabla f(x_k),x_k-u\rangle \\
=&\alpha\langle\nabla f(x_k),x_k-x_{k+1}\rangle+\alpha\langle\nabla f(x_k),x_{k+1}-u\rangle \\
=&\alpha\langle\nabla f(x_k),x_k-x_{k+1}\rangle-\langle\nabla V_{x_k}(x_{k+1}),x_{k+1}-u\rangle \\
=&\alpha\langle\nabla f(x_k),x_k-x_{k+1}\rangle-V_{x_{k+1}}(u)-V_{x_k}(x_{k+1})+V_{x_k}(u) \\
\le&\alpha\langle\nabla f(x_k),x_k-x_{k+1}\rangle-\frac{1}{2}\|x_k-x_{k+1}\|^2-V_{x_{k+1}}(u)+V_{x_k}(u) \\
\le&\frac{\alpha^2}{2}\|\nabla f(x_k)\|^2+V_{x_k}(u)-V_{x_{k+1}}(u) \\
\end{aligned}
$$
Telescoping sum over $k=0,1,\ldots,T-1$, let $\overline{x}=\dfrac{1}{T}\sum_{k=1}^Tx_k,u=x^*=\arg\min f(x)$
$$\implies\alpha T(f(\overline{x})-f(x^*))\le\alpha\left(\sum_{k=1}^Tf(x_k)-Tf(x^*)\right)\le\frac{\alpha^2}{2}\sum_{k=0}^{T-1}\|\nabla f(x_k)\|^2+V_{x_0}(x^*)$$
If $f$ is $\rho$-Lipschitz continuous, then $f(\overline{x})-f(x^*)\le\dfrac{\alpha\rho^2}{2}+\dfrac{V_{x_0}(x^*)}{\alpha T}$. Setting $\dfrac{\alpha\rho^2}{2}=\dfrac{V_{x_0}(x^*)}{\alpha T}=\dfrac{\epsilon}{2}$ gives
$$\alpha=\frac{\epsilon}{\rho^2},\quad T=\frac{2\rho^2V_{x_0}(x^*)}{\epsilon^2}\implies f(\overline{x})-f(x^*)\le\epsilon$$

</details>

## <span style="color: #F2A07B">Lec 04 Matrix Completion</span>

### <span style="color: #6ED3C5">Matrix Completion</span>

- given a partially observed matrix, recover the missing entries
- <span style="color: #6FA8FF">**low-rank assumption:**</span> the underlying complete matrix is low-rank
- <span style="color: #6FA8FF">**convex relaxation:**</span> (e.g., <span style="color: #6FA8FF">**nuclear norm minimization**</span>) relax the rank constraint to a convex problem (e.g., minimize nuclear norm, sum of singular values, subject to data constraints)
- <span style="color: #6FA8FF">**matrix factorization:**</span> decompose the matrix into product of two low-rank matrices
- <span style="color: #6FA8FF">**alternating minimization:**</span> iteratively update the low-rank factors
  - $\Omega$: set of observed entries
  - $P_\Omega(M)$: projection of matrix $M$ onto observed entries
  - objective: minimize $\|P_\Omega(XY^T)-P_\Omega(M)\|_F^2$ over $X,Y\in\mathbb{R}^{n\times r}$
  - for $t=1,2,\ldots,T$
    - fix $Y_{t-1}$, update $X_t$ by solving $\min_X\|P_\Omega(XY_{t-1}^T)-P_\Omega(M)\|_F^2$
    - fix $X_t$, update $Y_t$ by solving $\min_Y\|P_\Omega(X_tY^T)-P_\Omega(M)\|_F^2$
- singular value thresholding: apply soft-thresholding to singular values

for stationary points $\nabla f(x)=0$

- if $\nabla^2f(x)\succ 0$, local minimum
- if $\nabla^2f(x)\prec 0$, local maximum
- if $\nabla^2f(x)$ has both positive and negative eigenvalues, <span style="color: #6FA8FF">**strict saddle point**</span>
- if $\nabla^2f(x)\succeq 0$, local minimum or <span style="color: #6FA8FF">**flat saddle point**</span>

$f$ is <span style="color: #6FA8FF">**strict saddle**</span> if it does not have any flat saddle points
